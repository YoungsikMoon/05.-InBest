{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 한글 깨짐 방지\n",
    "# https://velog.io/@redgreen/Linux-linux%EC%97%90%EC%84%9C-Matplotlib-%ED%95%9C%EA%B8%80%ED%8F%B0%ED%8A%B8-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0\n",
    "\n",
    "import matplotlib.font_manager\n",
    "font_list = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "[matplotlib.font_manager.FontProperties(fname=font).get_name() for font in font_list if 'Nanum' in font]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumGothicCoding')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realUp</th>\n",
       "      <th>summ_context_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      realUp  summ_context_len\n",
       "2699       0               251\n",
       "2700       1               224\n",
       "2701       0               174\n",
       "2702       0               169\n",
       "2703       0               166"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # CSV 파일 읽기\n",
    "# summ_df = pd.read_csv('/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3.csv')\n",
    "\n",
    "# columns = ['realUp', 'stock', 'title', 'summ_context_len', 'ori_context_len', 'diff_len']\n",
    "# temp_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# for i in range(len(summ_df)):\n",
    "#     temp_df.loc[i, 'realUp'] = summ_df.loc[i, 'realUp']\n",
    "#     temp_df.loc[i, 'stock'] = summ_df.loc[i, 'stock']\n",
    "#     temp_df.loc[i, 'title'] = summ_df.loc[i, 'title']\n",
    "#     temp_df.loc[i, 'summ_context_len'] = len(summ_df.loc[i, 'summ_context'])\n",
    "#     temp_df.loc[i, 'ori_context_len'] = len(summ_df.loc[i, 'ori_context'])\n",
    "#     temp_df.loc[i, 'diff_len'] =  temp_df.loc[i, 'summ_context_len'] - temp_df.loc[i, 'ori_context_len']\n",
    "# temp_df.tail()\n",
    "\n",
    "\n",
    "# 개선된 코드\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "summ_df = pd.read_csv('/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-1.csv')\n",
    "\n",
    "# 필요한 열만 선택하여 새로운 데이터프레임 생성\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "# 각 열에 대해 apply 사용\n",
    "temp_df['realUp'] = summ_df['realUp']\n",
    "# temp_df['stock'] = summ_df['stock']\n",
    "# temp_df['title'] = summ_df['title']\n",
    "temp_df['summ_context_len'] = summ_df['summ_context'].apply(len)\n",
    "# temp_df['ori_context_len'] = summ_df['ori_context'].apply(len)\n",
    "\n",
    "# diff_len 계산\n",
    "# temp_df['diff_len'] = temp_df['summ_context_len'] - temp_df['ori_context_len']\n",
    "\n",
    "# 결과 확인\n",
    "display(temp_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 원본 기사 글자 수 분포\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# summ_df['ori_context'].str.len().hist(bins=30, color='pink')  # 막대 색상 지정\n",
    "# plt.title('원본 기사 글자 수 분포')\n",
    "# plt.xlabel('글자 수')\n",
    "# plt.ylabel('빈도')\n",
    "# plt.show()\n",
    "\n",
    "# 요약 기사 글자 수 분포\n",
    "plt.figure(figsize=(8, 4))\n",
    "summ_df['summ_context'].str.len().hist(bins=30, color='lightgreen')  # 막대 색상 지정\n",
    "plt.title('요약 기사의 글자 수 분포')\n",
    "plt.xlabel('글자 수')\n",
    "plt.ylabel('빈도')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 라벨 분포\n",
    "value_counts = summ_df['realUp'].value_counts()\n",
    "total_count = value_counts.sum()\n",
    "plt.figure(figsize=(8, 4))\n",
    "value_counts.plot(kind='bar', color=['red', 'blue'])  # 막대 색상 지정\n",
    "plt.title('라벨 분포')\n",
    "plt.xlabel('realUp')\n",
    "plt.ylabel('Count')\n",
    "# 총 카운트 수 표시\n",
    "plt.text(0, 1.2, f'Total Count: {total_count}', transform=plt.gca().transAxes, ha='center', va='top')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "realUp\n",
       "0    1661\n",
       "1    1043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_df['realUp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "75\n",
      "92\n",
      "89\n",
      "40\n",
      "93\n",
      "99\n",
      "84\n",
      "93\n",
      "59\n",
      "79\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(temp_df)):\n",
    "    if temp_df.loc[i, 'summ_context_len'] < 100 : # 뉴스 내용이 너무 적어서 학습에 방해될 것 3개\n",
    "        print(temp_df.loc[i, 'summ_context_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      realUp                                       summ_context\n",
      "104        0  코스닥지수가 외국인 순매수에 힘입어 일 연속 상승세를 이어가며 선을 회복했습니다 의...\n",
      "151        0  정부의 북한 조림 사업 추진 계획이 기대되면서 조림 관련 종목이 초반 상승세를 보였...\n",
      "210        0  한국 코스피지수는 외국인 순매수로 인해 상승하여 로 마감하였습니다 삼성전자 하이닉스...\n",
      "368        0  휴네시온 이 상한가를 기록 중이며 현재 원에 거래되고 있습니다 최근 거래량이 급증했...\n",
      "562        0           네오오토 장 시작부터 높은 상승세 보여 오전 시 분 현재 전일 대비 증가\n",
      "739        0  케이엠제약이 시작부터 강한 상승세를 보이며 장중 최고 까지 올랐다가 현재는 상승한 ...\n",
      "1141       1  한국거래소 코스닥시장본부가 유니온커뮤니티의 주가 급등에 대해 조회공시를 요구하고 있...\n",
      "1143       1  휴맥스는 연결 기준으로 올해 분기 영업이익이 지난해 동기 대비 감소했다고 공시했습니...\n",
      "1189       0  오늘 코스피 지수는 외국인 매도에도 불구하고 개인이 꾸준히 순매수하면서 상승해 에 ...\n",
      "1396       1  한국 코스닥지수 상승 외국인 및 기관투자자 동반 매수세 오락 문화 관련주가 강세 키...\n",
      "1707       0  주요 내용 요약 코스닥 지수가 거래일 만에 하락하여 개인 투자자의 순매수로 간신히 ...\n",
      "2009       0  일 코스피 지수 전 거래일 대비 상승하며 로 마감 외국인 및 기관이 매수를 이어가며...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "temp_df['realUp'] = summ_df[['realUp']]\n",
    "temp_df['summ_context'] = summ_df[['summ_context']]\n",
    "\n",
    "# summ_context의 길이가 00 미만인 행 필터링\n",
    "filtered_df = temp_df[temp_df['summ_context'].str.len() <100]\n",
    "\n",
    "# 결과 출력\n",
    "print(filtered_df) #60 미만 부터 이유가 없음\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "filtered_df.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   realUp                                       summ_context\n",
      "0       1  국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승...\n",
      "1       1  우리기술투자 가 가상화폐 거래소인 업비트를 운영하는 두나무 지분 가치가 주목받고 있...\n",
      "2       1  뉴프라이드 는 미국 대마초 판매 합법화에 따라 코스닥 시장에서 상승세를 보이고 있습...\n",
      "3       0  요약하자면 코스피가 연속 상승세를 이어가며 사상 최고치인 선을 향해 가고 있고 코스...\n",
      "4       1  코스닥지수는 바이오 제약주들의 차익 실현 매물로 인해 하락세를 보였습니다 기관은 대...\n"
     ]
    }
   ],
   "source": [
    "# summ_context의 길이가 60 미만인 행 삭제\n",
    "temp_df = temp_df[temp_df['summ_context'].str.len() >= 100]\n",
    "print(temp_df.head())\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      realUp                                       summ_context\n",
      "0          1  국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승...\n",
      "1          1  우리기술투자 가 가상화폐 거래소인 업비트를 운영하는 두나무 지분 가치가 주목받고 있...\n",
      "2          1  뉴프라이드 는 미국 대마초 판매 합법화에 따라 코스닥 시장에서 상승세를 보이고 있습...\n",
      "3          0  요약하자면 코스피가 연속 상승세를 이어가며 사상 최고치인 선을 향해 가고 있고 코스...\n",
      "4          1  코스닥지수는 바이오 제약주들의 차익 실현 매물로 인해 하락세를 보였습니다 기관은 대...\n",
      "...      ...                                                ...\n",
      "2687       0  해당 기사는 코스의 지수가 미국 금리 하락 기대가 줄어들고 이른 폭염으로 인해 빙과...\n",
      "2688       1  코스닥 지수는 외국인 매도로 인해 거래일 만에 하락 마감했습니다 코스닥은 오후 한때...\n",
      "2689       0  코스닥 지수가 상승세를 보이며 전거래일 대비 오른 로 거래 중입니다 나스닥발 훈풍으...\n",
      "2690       0  라씨로는 기반 주식 분석 정보를 제공하는 전자신문 증권 정보 애플리케이션입니다 플레...\n",
      "2691       0  증권은 삼양식품의 주요 고객사인 식품소재 기업 에스앤디 가 불닭볶음면 열풍으로 실적...\n",
      "\n",
      "[2692 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "temp_df = pd.read_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-2.csv')\n",
    "\n",
    "# 한글만 남기고 나머지 제거하는 함수 정의\n",
    "def keep_hangul(text):\n",
    "    return ' '.join(re.findall(r'[가-힣]+', text))\n",
    "\n",
    "# summ_context 열에 한글만 남기기\n",
    "temp_df['summ_context'] = temp_df['summ_context'].apply(keep_hangul)\n",
    "\n",
    "# 결과 출력\n",
    "print(temp_df)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: 안녕하세요 오늘의 뉴스 요약입니다.\n",
      "KoGPT2 토큰화된 결과: ['▁안녕', '하', '세', '요', '▁오늘의', '▁뉴스', '▁요약', '입니다.']\n",
      "KoBERT 토큰화된 결과: ['안녕하세요', '오늘의', '뉴스', '요약', '##입니다', '.']\n",
      "Okt_nouns 토큰화된 결과: ['오늘', '뉴스', '요약']\n",
      "Okt_morphs 토큰화된 결과: ['안녕하세요', '오늘', '의', '뉴스', '요약', '입니다', '.']\n",
      "KoGPT2 토큰 ID: [25906, 8702, 7801, 8084, 34461, 26506, 17669, 21154]\n",
      "KoBERT 토큰 ID: [16453, 17622, 21585, 26539, 13992, 2016]\n",
      "KoGPT2 디코딩된 텍스트: 안녕하세요 오늘의 뉴스 요약입니다.\n",
      "KoBERT 디코딩된 텍스트: 안녕하세요 오늘의 뉴스 요약입니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast # KoGPT2\n",
    "from transformers import BertTokenizer # KoBERT\n",
    "from konlpy.tag import Okt # okt\n",
    "\n",
    "\n",
    "# KoGPT 모델에 맞는 토크나이저 로드\n",
    "tokenizer_KoGPT = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "# KoBERT 모델에 맞는 토크나이저 로드\n",
    "tokenizer_KoBERT = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "# OKT\n",
    "okt = Okt()\n",
    "\n",
    "# 토큰화할 텍스트\n",
    "text = \"안녕하세요 오늘의 뉴스 요약입니다.\"\n",
    "\n",
    "# 텍스트를 토큰화\n",
    "tokens_KoGPT = tokenizer_KoGPT.tokenize(text)\n",
    "tokens_KoBERT =tokenizer_KoBERT.tokenize(text)\n",
    "\n",
    "# 토큰 ID 변환\n",
    "token_ids_KoGPT = tokenizer_KoGPT.convert_tokens_to_ids(tokens_KoGPT)\n",
    "token_ids_KoBERT =tokenizer_KoBERT.convert_tokens_to_ids(tokens_KoBERT)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"원본 텍스트:\", text)\n",
    "print(\"KoGPT2 토큰화된 결과:\", tokens_KoGPT)\n",
    "print(\"KoBERT 토큰화된 결과:\", tokens_KoBERT)\n",
    "print(\"Okt_nouns 토큰화된 결과:\", okt.nouns(text))\n",
    "print(\"Okt_morphs 토큰화된 결과:\", okt.morphs(text))\n",
    "print(\"KoGPT2 토큰 ID:\", token_ids_KoGPT)\n",
    "print(\"KoBERT 토큰 ID:\", token_ids_KoBERT)\n",
    "\n",
    "# 토큰 ID를 다시 텍스트로 변환\n",
    "decoded_text_KoGPT = tokenizer_KoGPT.decode(token_ids_KoGPT)\n",
    "decoded_text_KoBERT = tokenizer_KoBERT.decode(token_ids_KoBERT)\n",
    "print(\"KoGPT2 디코딩된 텍스트:\", decoded_text_KoGPT)\n",
    "print(\"KoBERT 디코딩된 텍스트:\", decoded_text_KoBERT)\n",
    "\n",
    "# 원본 텍스트: 안녕하세요, 오늘의 뉴스 요약입니다.\n",
    "# KoGPT2 토큰화된 결과: ['▁안녕', '하', '세', '요,', '▁오늘의', '▁뉴스', '▁요약', '입니다.']\n",
    "# KoBERT 토큰화된 결과: ['안녕하세요', ',', '오늘의', '뉴스', '요약', '##입니다', '.']\n",
    "# Okt_nouns 토큰화된 결과: ['오늘', '뉴스', '요약']\n",
    "# Okt_morphs 토큰화된 결과: ['안녕하세요', ',', '오늘', '의', '뉴스', '요약', '입니다', '.']\n",
    "# KoGPT2 토큰 ID: [25906, 8702, 7801, 13704, 34461, 26506, 17669, 21154]\n",
    "# KoBERT 토큰 ID: [16453, 2014, 17622, 21585, 26539, 13992, 2016]\n",
    "# KoGPT2 디코딩된 텍스트: 안녕하세요, 오늘의 뉴스 요약입니다.\n",
    "# KoBERT 디코딩된 텍스트: 안녕하세요, 오늘의 뉴스 요약입니다.\n",
    "\n",
    "# 토큰화는 Okt_nouns가 좋을것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okt nouns 처리한 파일이 'naver_news_origin_duplicates_summ3-4.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter  # Counter 클래스 임포트\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 토크나이저 로드\n",
    "okt = Okt()\n",
    "\n",
    "# summ_context 열에 Okt 토크나이저 적용 및 1글자 이하의 토큰 삭제\n",
    "temp_df['okt_tokenized'] = temp_df['summ_context'].apply(\n",
    "    lambda x: [word for word in okt.nouns(x) if len(word) > 1]  # 1글자 이하 토큰 필터링\n",
    ")\n",
    "\n",
    "# 토큰화 데이터프레임 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-4.csv')\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"okt nouns 처리한 파일이 'naver_news_origin_duplicates_summ3-4.csv'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모든 토큰을 하나의 리스트로 결합\n",
    "# all_tokens = [token for sublist in temp_df['okt_tokenized'] for token in sublist]\n",
    "\n",
    "# # 단어 빈도수 계산\n",
    "# word_counts = Counter(all_tokens)\n",
    "\n",
    "# # 빈도수 내림차순 정렬\n",
    "# sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 결과를 데이터프레임으로 변환\n",
    "# word_freq_df = pd.DataFrame(sorted_word_counts, columns=['단어', '빈도수'])\n",
    "\n",
    "# # 결과를 CSV 파일로 저장\n",
    "# word_freq_df.to_csv('word_frequency.csv', index=False)\n",
    "\n",
    "# # 저장 완료 메시지 출력\n",
    "# print(\"단어 빈도수 파일이 'word_frequency.csv'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (희소문제, 단어 순서 무시, 문맥 정보 부족, 문서 길이 편향, 어휘 확장성 부족) 사용 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 데이터프레임:\n",
      "       가가        가격   가결   가공  가공업   가구  가까이   가능       가능성   가닥  ...   희소  \\\n",
      "0     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "1     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.100542  0.0  ...  0.0   \n",
      "2     0.0  0.113091  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "3     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "4     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "...   ...       ...  ...  ...  ...  ...  ...  ...       ...  ...  ...  ...   \n",
      "2687  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2688  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2689  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2690  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2691  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "\n",
      "      희소식  희토류   히터   히트  히트텍  힌남노   힐링  힘겨루기   힘스  \n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...   ...  ...  \n",
      "2687  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2688  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2689  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2690  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2691  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[2692 rows x 7945 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast  # 문자열을 리스트로 변환하기 위한 모듈\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 문자열 형태의 리스트를 실제 리스트로 변환\n",
    "temp_df['okt_tokenized'] = temp_df['okt_tokenized'].apply(ast.literal_eval)\n",
    "\n",
    "# 토큰 리스트를 문자열로 변환\n",
    "temp_df['text'] = temp_df['okt_tokenized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(temp_df['text'])\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"TF-IDF 데이터프레임:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# TF-IDF 벡터화기 저장\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "# TF-IDF 매트릭스 저장 (필요한 경우)\n",
    "tfidf_df.to_csv('tfidf_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 데이터에 대한 TF-IDF 데이터프레임:\n",
      "    가가   가격   가결   가공  가공업   가구  가까이   가능  가능성   가닥  ...   희소  희소식  희토류   히터  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "    히트  히트텍  힌남노   힐링  힘겨루기   힘스  \n",
      "0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[2 rows x 7945 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 새로운 데이터 예시\n",
    "new_data = [\"안녕하세요 나는 문영식 입니다 벡터화 테스트 입니다\", \"어쩔탱 저쩔탱 쿠쿠르삥뽕 안물 안궁\"]\n",
    "\n",
    "# TF-IDF 벡터화기 로드\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "\n",
    "# 새로운 데이터를 DataFrame으로 변환\n",
    "new_df = pd.DataFrame(new_data, columns=['text'])\n",
    "\n",
    "# 새로운 데이터에 대해 TF-IDF 벡터화\n",
    "new_tfidf_matrix = loaded_vectorizer.transform(new_df['text'])\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "new_tfidf_df = pd.DataFrame(new_tfidf_matrix.toarray(), columns=loaded_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"새로운 데이터에 대한 TF-IDF 데이터프레임:\")\n",
    "print(new_tfidf_df)\n",
    "\n",
    "# 필요시 새로운 TF-IDF 결과를 CSV 파일로 저장\n",
    "new_tfidf_df.to_csv('new_tfidf_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ko_BERT로 임베딩해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과:\n",
      "{'input_ids': tensor([[    2,  8982,  3933,  9415, 14866,  8535,  9246, 11608,  5138,  9154,\n",
      "          9389,  5167,  2953, 10464,  5105, 14295, 11539, 10999,  3599,  5817,\n",
      "         10695, 10026, 14589,  9594, 19509,  5284, 11329,  5816,  8667,  9767,\n",
      "          3683,  8932,  8961,  9445,  1998,  5238, 13322,  8513,  9000,  9566,\n",
      "          5310,  5138,  3683, 10741,  1920, 19501, 17821,  9594,  3824,  5345,\n",
      "          9396,  5351,  5284, 17048, 18499, 15905,  8454,  1932, 11000, 17373,\n",
      "          5072, 10983, 10464,  5019,  3322,  5550, 10460, 12711,  9255,  8538,\n",
      "         10624,  8465,  3682, 17996,  1932, 13658,  5067, 11953,  9602, 12424,\n",
      "          5051,  8486, 14866,  8535, 11608,  5008,  9751,  8455,  9594,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    2,  8472, 10984, 12148,  1920, 17106, 19319, 10253,  5208,  5096,\n",
      "          3576,  5444, 11385, 16135,  2500, 17566, 16429, 14932, 13279,  9177,\n",
      "          9074,  2088,  5506,  8455,  9594,  3819,  5147,  5655,  3707,  5166,\n",
      "         12148, 15673,  9536,  9483,  3576,  5444, 18532,  3749, 10253, 19285,\n",
      "          5022,  3546, 18087,  8452,  9054,  5214,  5678, 16412, 11480,  5022,\n",
      "          3546,  3566,  5127,  5037,  8804,  4137,  5132,  9066,  8538, 15363,\n",
      "          5105, 16429, 11922,  5035, 12344,  8758, 10149, 10504,  5088,  8804,\n",
      "         10302, 15123, 17106, 19319,  9246,  5019,  9973,  9389,  5178, 10014,\n",
      "         10384, 11606, 14196,  5072, 11608, 10247,     3,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    2,  2361, 16443, 12026,  2367,  8523, 16959,  5817,  9779, 13430,\n",
      "         15318,  8758, 11329,  5816, 18642, 10695, 10026, 14589,  9594,  2361,\n",
      "         16443, 12026,  5035, 13587,  3778, 16244,  2571,  3209, 13600,  5029,\n",
      "          9085, 13322,  9173, 11344,  3382,  3064, 12776,  3682,  5051, 10253,\n",
      "          8926, 10983, 10534,  3807,  5037, 10862, 10877,  9472,  8574, 11088,\n",
      "         16565, 10695,  5019,  8523,  4189,  5097,  5485, 14596, 14621,  5074,\n",
      "          2099,  5147,  5052, 16959,  5817,  9779,  5040, 13430,  5149,  9305,\n",
      "          5035, 16171,  5051, 11246, 12738,  3742,  3659,  5345,  5019,  2361,\n",
      "         16443, 12026,  5105,  9833,  9246, 11597,  9223, 11329,  5816, 18642,\n",
      "         18602, 18551,  5062, 10695, 13658,  4112,  9790, 15748,  5072,  9594,\n",
      "         16030, 16565,  5040, 13587,  3209, 13600,  5029,  9085, 13322, 10534,\n",
      "          4189,  5097,  5485, 14596, 14621,  5074,  2099,  5147,  5052, 16959,\n",
      "          5817,  9779,  5040, 13430,  5149,  9305,  5035,  9371,  5016, 10999,\n",
      "         11578,  5166, 12738,  3659,  5345,  5019,  2361, 16443, 12026,  5105,\n",
      "          8988,  9833,  9472,  5008, 19185, 18629,  8455, 10983,  3742,  9711,\n",
      "          5040,  8936,  5331,  9046, 11922,  8494, 13909,  9957,  8736,  1923,\n",
      "          5278,  8455,  8745,  8591, 15420,  4565,  9439, 11825, 19184,  5412,\n",
      "          9020,     3],\n",
      "        [    2,  3659,  5345,  8799,  5093, 19509,  5040, 11799, 10695, 10026,\n",
      "          9012, 12877, 11330,  8961,  5244,  5096,  3245,  5008, 14585, 11081,\n",
      "          9046, 11329,  5816,  5019,  3778,  5213,  3245, 14635,  5029, 12115,\n",
      "          3898,  8574,  8523,  9711, 10984,  9931,  5138,  4090,  5377,  3576,\n",
      "         14544,  1932,  5189,  5284,  8888, 10322,  1923, 12831, 13906, 19017,\n",
      "         16171,  5051,  4647,  5441,  5043, 10322,  1998,  5389,  8936, 16565,\n",
      "          8456, 10695, 10026,  3030,  5466,  8538, 10464,  5105,  3322,  5550,\n",
      "          9568,  3940,  9449, 10555, 12183,  8529,  9594,     3,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    2, 11329,  5816,  5034,  9568,  9396,  5351,  3824,  5345,  5020,\n",
      "          8545,  4059,  5648, 15153,  2821, 15332, 10999, 11757, 10026,  3030,\n",
      "          5466,  8538, 10570,  5019, 14712,  5016,  3322,  5550,  5067, 13322,\n",
      "         10464,  9389,  9766,  3286,  5514,  5105,  3322,  5550,  9449,  9217,\n",
      "         12304, 11329,  5816,  9512,  8454,  3382,  5040,  5571,  5470, 13497,\n",
      "         19961,  8465,  9376,  8519, 18766,  5062, 10968,  3030,  5466,  8809,\n",
      "         17267,  5824,  5022,  8611,  2088,  5645,  9066,  8538, 11860, 17106,\n",
      "         19319,  4317,  5224, 14621, 12153, 10695, 10026,  3030,  5466,  8538,\n",
      "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")\n",
    "\n",
    "# 토큰화 예제\n",
    "def tokenize_sentences(sentences):\n",
    "    # 문장들을 토큰화\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# 예시 문장 토큰화\n",
    "sample_sentences = df['summ_context'].tolist()[:5]  # 첫 5개 문장 예시\n",
    "tokenized_output = tokenize_sentences(sample_sentences)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"토큰화 결과:\")\n",
    "print(tokenized_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ko_bert 토크나이저를 쓰면 임베딩 잘 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핀 버트 토크나이저 + 로지스틱리그레션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['summ_context'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['summ_context'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")\n",
    "print(\"분류 리포트:\\n\", report)\n",
    "\n",
    "\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert\", num_labels=2)\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_encodings = tokenizer(train_df['summ_context'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['summ_context'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, y_train)\n",
    "test_dataset = NewsDataset(test_encodings, y_test)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,  # 로그를 더 자주 출력하도록 설정\n",
    "    evaluation_strategy=\"steps\",  # 평가 전략을 설정\n",
    "    eval_steps=100,  # 평가를 위한 스텝 수\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# 평가지표 함수 정의\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='binary')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# 조기 중단 콜백 추가\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Trainer를 통한 모델 훈련\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],  # 조기 중단 콜백 추가\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 예측 및 평가\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")\n",
    "print(\"분류 리포트:\\n\", report)\n",
    "\n",
    "\n",
    "# Step\tTraining Loss\tValidation Loss\tAccuracy\tF1\tPrecision\tRecall\n",
    "# 100\t0.673700\t0.682647\t0.580705\t0.058333\t0.466667\t0.031111\n",
    "# 200\t0.672600\t0.738892\t0.582560\t0.000000\t0.000000\t0.000000\n",
    "# 300\t0.655900\t0.697149\t0.573284\t0.361111\t0.481481\t0.288889\n",
    "# 400\t0.627100\t0.747127\t0.564007\t0.192440\t0.424242\t0.124444\n",
    "# 500\t0.434400\t0.982481\t0.554731\t0.329609\t0.443609\t0.262222\n",
    "# 600\t0.289800\t1.911037\t0.517625\t0.389671\t0.412935\t0.368889\n",
    "# 700\t0.190600\t2.163080\t0.556586\t0.366048\t0.453947\t0.306667\n",
    "# 800\t0.120500\t2.438479\t0.541744\t0.384040\t0.437500\t0.342222\n",
    "# 900\t0.075200\t2.769468\t0.551020\t0.434579\t0.458128\t0.413333\n",
    "# 1000\t0.040300\t3.182628\t0.560297\t0.328612\t0.453125\t0.257778\n",
    "# 1100\t0.053500\t3.084835\t0.530612\t0.423690\t0.434579\t0.413333\n",
    "# 1200\t0.019700\t3.810341\t0.552876\t0.244514\t0.414894\t0.173333\n",
    "# 1300\t0.017800\t3.791117\t0.545455\t0.350133\t0.434211\t0.293333\n",
    "# 1400\t0.030000\t3.805593\t0.551020\t0.366492\t0.445860\t0.311111\n",
    "# 1500\t0.048100\t3.526658\t0.551020\t0.431925\t0.457711\t0.408889\n",
    "# 1600\t0.003800\t3.807347\t0.536178\t0.407583\t0.436548\t0.382222\n",
    "# 1700\t0.002800\t3.892785\t0.551020\t0.473913\t0.463830\t0.484444\n",
    "# 1800\t0.014300\t3.804846\t0.558442\t0.466368\t0.470588\t0.462222\n",
    "# 1900\t0.016100\t3.954975\t0.538033\t0.430206\t0.443396\t0.417778\n",
    "# 2000\t0.006300\t4.082551\t0.504638\t0.483559\t0.428082\t0.555556\n",
    "# 2100\t0.001000\t4.174303\t0.536178\t0.338624\t0.418301\t0.284444\n",
    "# 2200\t0.000200\t4.217677\t0.530612\t0.412993\t0.432039\t0.395556\n",
    "# 2300\t0.011200\t4.336666\t0.517625\t0.462810\t0.432432\t0.497778\n",
    "# 2400\t0.001000\t4.378465\t0.532468\t0.444934\t0.441048\t0.448889\n",
    "# 2500\t0.001800\t4.347987\t0.541744\t0.434783\t0.448113\t0.422222\n",
    "# 2600\t0.000300\t4.394334\t0.541744\t0.399027\t0.440860\t0.364444\n",
    "# 2700\t0.009800\t4.398994\t0.549165\t0.362205\t0.442308\t0.306667\n",
    "# 2800\t0.000000\t4.432846\t0.556586\t0.359249\t0.452703\t0.297778\n",
    "# 2900\t0.000100\t4.375212\t0.536178\t0.436937\t0.442922\t0.431111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okt + 핀버트토크나이저 + 로지스틱리그레션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-4.csv'\n",
    "temp_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리한 파일이 'naver_news_origin_duplicates_summ3-5.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# okt 토큰화 된 리스트 -> 문자열로 전환\n",
    "temp_df['okt_tokenized_str'] = temp_df['okt_tokenized'].apply(lambda x: ''.join(x).strip('[]').replace(',', '').replace('\\'', ''))\n",
    "\n",
    "# 토큰화 데이터프레임 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv')\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"처리한 파일이 'naver_news_origin_duplicates_summ3-5.csv'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5529\n",
      "분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.86      0.69       314\n",
      "           1       0.39      0.12      0.19       225\n",
      "\n",
      "    accuracy                           0.55       539\n",
      "   macro avg       0.48      0.49      0.44       539\n",
      "weighted avg       0.50      0.55      0.48       539\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['okt_tokenized_str', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['okt_tokenized_str'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['okt_tokenized_str'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")\n",
    "print(\"분류 리포트:\\n\", report)\n",
    "\n",
    "# finbert 토크나이저만 적용한 점수\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n",
    "\n",
    "# Okt + finbert토크나이저 적용한 점수 (더 안좋아졌다.)\n",
    "# 정확도와 프리시전은 미미하게 낮아지고, 0에 대한 리콜과 f1은 오르고 1에대한 점수는 대폭 낮아졌다. \n",
    "# 정확도: 0.5529\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.86      0.69       314\n",
    "#            1       0.39      0.12      0.19       225\n",
    "\n",
    "#     accuracy                           0.55       539\n",
    "#    macro avg       0.48      0.49      0.44       539\n",
    "# weighted avg       0.50      0.55      0.48       539\n",
    "\n",
    "# 결론 : 0이라고 예측을 많이하고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 정확도: 0.5380\n",
      "XGBoost 분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67       314\n",
      "           1       0.39      0.18      0.25       225\n",
      "\n",
      "    accuracy                           0.54       539\n",
      "   macro avg       0.48      0.49      0.46       539\n",
      "weighted avg       0.50      0.54      0.49       539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['summ_context'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['summ_context'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "xgb_y_pred = xgb_model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost 정확도: {xgb_accuracy:.4f}\")\n",
    "print(\"XGBoost 분류 리포트:\\n\", xgb_report)\n",
    "\n",
    "\n",
    "# finbert토크나이저 + 로지스틱리그레션  적용한 점수\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n",
    "\n",
    "# Okt + finbert토크나이저 + 로지스틱리그레션 적용한 점수 (더 안좋아졌다.)\n",
    "# 정확도와 프리시전은 미미하게 낮아지고, 0에 대한 리콜과 f1은 오르고 1에대한 점수는 대폭 낮아졌다. \n",
    "# 정확도: 0.5529\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.86      0.69       314\n",
    "#            1       0.39      0.12      0.19       225\n",
    "\n",
    "#     accuracy                           0.55       539\n",
    "#    macro avg       0.48      0.49      0.44       539\n",
    "# weighted avg       0.50      0.55      0.48       539\n",
    "\n",
    "# finbert토크나이저 + XG부스팅\n",
    "# XGBoost 정확도: 0.5380\n",
    "# XGBoost 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.79      0.67       314\n",
    "#            1       0.39      0.18      0.25       225\n",
    "\n",
    "#     accuracy                           0.54       539\n",
    "#    macro avg       0.48      0.49      0.46       539\n",
    "# weighted avg       0.50      0.54      0.49       539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 정확도: 0.5603\n",
      "XGBoost 분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.84      0.69       314\n",
      "           1       0.43      0.16      0.24       225\n",
      "\n",
      "    accuracy                           0.56       539\n",
      "   macro avg       0.51      0.50      0.46       539\n",
      "weighted avg       0.52      0.56      0.50       539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['okt_tokenized_str', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['okt_tokenized_str'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['okt_tokenized_str'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "xgb_y_pred = xgb_model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost 정확도: {xgb_accuracy:.4f}\")\n",
    "print(\"XGBoost 분류 리포트:\\n\", xgb_report)\n",
    "\n",
    "\n",
    "# finbert토크나이저 + 로지스틱리그레션  적용한 점수\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n",
    "\n",
    "# Okt + finbert토크나이저 + 로지스틱리그레션\n",
    "# 정확도: 0.5529\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.86      0.69       314\n",
    "#            1       0.39      0.12      0.19       225\n",
    "\n",
    "#     accuracy                           0.55       539\n",
    "#    macro avg       0.48      0.49      0.44       539\n",
    "# weighted avg       0.50      0.55      0.48       539\n",
    "\n",
    "# finbert토크나이저 + XG부스팅\n",
    "# XGBoost 정확도: 0.5380\n",
    "# XGBoost 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.79      0.67       314\n",
    "#            1       0.39      0.18      0.25       225\n",
    "\n",
    "#     accuracy                           0.54       539\n",
    "#    macro avg       0.48      0.49      0.46       539\n",
    "# weighted avg       0.50      0.54      0.49       539\n",
    "\n",
    "# Okt + finbert토크나이저 + XG부스팅\n",
    "# XGBoost 정확도: 0.5603\n",
    "# XGBoost 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.84      0.69       314\n",
    "#            1       0.43      0.16      0.24       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.51      0.50      0.46       539\n",
    "# weighted avg       0.52      0.56      0.50       539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ollama 임베딩은 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승세를 보이고 있습니다 코스피와 코스닥 모두 지난해 월 이후 최고치를 경신했으며 정부 정책 기대감과 월 효과 가 시장을 지지하고 있습니다 제약 바이오와 소프트웨어 부문에서 강세가 나타나고 있으며 외국인은 순매수로 전환되었습니다 전문가들은 원화의 강세에도 불구하고 수출 수요에 대한 긍정적인 전망을 유지하고 있습니다'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summ_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding': [-0.2482476532459259, -0.41597917675971985, 0.13944777846336365, 1.0526032447814941, 0.24063798785209656, -0.44049710035324097, -0.10877615213394165, 0.403290718793869, 0.45950672030448914, 0.5548033118247986, 0.38686761260032654, -0.5962243676185608, 0.8978961706161499, -0.933647632598877, -0.3009205758571625, -0.09831070899963379, -0.23973596096038818, -1.1432019472122192, 0.007715567946434021, 0.34611114859580994, -0.4141584634780884, -0.4365220367908478, -1.5286723375320435, -0.12845668196678162, -0.1325674206018448, 1.4207186698913574, -0.032718975096940994, 0.22066029906272888, 1.2345412969589233, 0.3435004949569702, -0.45251500606536865, -0.47138291597366333, -0.5420364737510681, -0.08158192038536072, -0.12600180506706238, -0.24234803020954132, 1.5605820417404175, -0.6401556134223938, 0.029555637389421463, -0.23564696311950684, -0.17311038076877594, -1.0523992776870728, 1.9090009927749634, -0.7583023309707642, -0.07076889276504517, -0.7441924214363098, -0.24335746467113495, -0.5352200865745544, 0.4577159285545349, -0.9738096594810486, 0.9674739837646484, 0.4378001093864441, 1.43301522731781, -0.5133544206619263, 0.3824057877063751, -0.033272095024585724, 0.4151143431663513, -0.31509262323379517, -0.18688279390335083, 0.5050758123397827, 0.7555458545684814, -0.5732021927833557, 0.5730724930763245, -0.5116720199584961, 0.6301442980766296, 0.2909775972366333, -0.09410136938095093, -0.11284034699201584, -0.021937210112810135, -0.46561163663864136, -0.3281863033771515, 0.6147173047065735, -0.06519707292318344, -0.2513508200645447, -0.22246479988098145, 0.007742300629615784, -0.34170767664909363, -0.0255635567009449, -0.0370473712682724, -0.19509929418563843, -0.20077955722808838, 0.383554071187973, -0.36718347668647766, 0.6330702304840088, -0.25368356704711914, -0.5529093146324158, 0.6195179224014282, -0.37361058592796326, -0.18714050948619843, -0.295110821723938, -0.1870080977678299, 0.3668830096721649, 0.18629634380340576, -0.556039571762085, 0.39803752303123474, 0.1478387415409088, -0.7270364165306091, 0.18686120212078094, -1.149728536605835, 0.26533037424087524, 0.6177219748497009, 0.9687349200248718, -1.2068214416503906, 1.4855841398239136, -0.6570420265197754, -0.20399849116802216, -0.03065311908721924, -0.13159386813640594, 0.6194908022880554, -0.5625587701797485, -0.00601109117269516, 0.33641910552978516, -0.16623282432556152, -0.19519749283790588, -1.0273969173431396, 0.9752483367919922, -0.055457115173339844, 0.2155287265777588, -0.9634296298027039, 0.07019402086734772, -0.5665355920791626, 0.7945084571838379, 0.5646507143974304, -0.1309797167778015, -0.24456685781478882, -0.09326928853988647, -0.5981773138046265, 0.6243931651115417, -1.1925179958343506, -0.4821915626525879, -0.1775607168674469, -0.03373491391539574, -0.3301040530204773, 0.2843787372112274, -0.54612797498703, -0.013241324573755264, -1.0882740020751953, 1.3445500135421753, 0.21007169783115387, -0.9664933085441589, 0.13383331894874573, -0.0891389474272728, -0.0853690356016159, 0.9338483214378357, 0.3609526753425598, 0.2356908917427063, 0.20918454229831696, -0.5549854636192322, -0.2771218419075012, 0.4360131621360779, -0.29766201972961426, 0.019883744418621063, -0.16190333664417267, -0.07419294863939285, 0.15278476476669312, -0.9165909290313721, 0.20568564534187317, -0.19571229815483093, 0.40861791372299194, 0.2429688274860382, -0.19931764900684357, 0.312783420085907, -0.6996430158615112, 0.559502124786377, -0.34447944164276123, 0.1605047583580017, 0.021350499242544174, -0.3356391191482544, -0.06786397099494934, -0.43922314047813416, 0.29821330308914185, 0.3924165666103363, -0.07625135779380798, 0.22461146116256714, 0.30644491314888, 1.2281346321105957, 0.28537070751190186, 0.5320498943328857, 0.6989068388938904, 0.3732302188873291, -0.6649776697158813, 0.6244534850120544, 0.08534242957830429, 0.41500726342201233, 0.4063340723514557, -0.15564078092575073, 0.14335289597511292, -0.11892618238925934, -0.5511278510093689, -0.5085996389389038, 0.05762866139411926, 1.109706997871399, -1.1791582107543945, -0.01304788887500763, 0.2106371819972992, 0.33906522393226624, 0.08630377054214478, -0.3535979092121124, -0.36512699723243713, -0.7442933917045593, -0.43429097533226013, 0.9918767213821411, -0.7223582863807678, 1.0352123975753784, 0.2337636947631836, -1.3756587505340576, 0.42276445031166077, 0.7841792106628418, -0.2753625810146332, 0.05800964683294296, 0.9487124085426331, 0.45658060908317566, 0.01576428860425949, -0.6367303133010864, 0.9800726771354675, -0.4525220990180969, -0.0976300984621048, 0.3621545433998108, -0.042383745312690735, -0.15474939346313477, -0.052717771381139755, 0.8458952307701111, -0.009705107659101486, -0.5255334973335266, 0.17620618641376495, 0.6947235465049744, 0.4492141306400299, 0.8336024880409241, 0.36207762360572815, -0.05967370793223381, -0.1283435970544815, 1.4772281646728516, -0.06960395723581314, 0.7479748725891113, 0.8597362041473389, 0.9990130662918091, 1.281172752380371, 0.8036050796508789, 0.2999217212200165, -0.06130853295326233, -0.24069815874099731, -0.04924778640270233, 1.1287343502044678, 0.7960525155067444, -0.6670475006103516, 0.4132782220840454, -0.12752878665924072, -0.3679170608520508, 0.21921303868293762, 0.20066578686237335, -0.3177039325237274, 1.2830697298049927, 0.8947442173957825, 0.32462573051452637, -0.6342706680297852, -0.5333991050720215, 0.9635683298110962, 1.071516990661621, -0.6103765368461609, 0.21773189306259155, 0.5002548098564148, -0.19100521504878998, 0.6049837470054626, 0.577227771282196, -0.16065776348114014, 0.4465070962905884, -0.660679817199707, -0.7795079350471497, -0.8212793469429016, -0.48565563559532166, -0.44341108202934265, -1.064011573791504, -0.6210508346557617, -0.34841451048851013, -0.519940197467804, 1.0015246868133545, 0.6778056025505066, -0.8415072560310364, 0.8116828799247742, -0.5107752680778503, -0.9192028045654297, -0.593417763710022, -0.18584245443344116, 0.5292366743087769, -0.019730230793356895, 0.23532940447330475, -0.35687288641929626, 0.1569330394268036, 0.42233094573020935, 1.85791015625, -0.5637195706367493, -0.011882428079843521, 0.17456023395061493, -0.024972040206193924, 0.10319912433624268, -0.6059674024581909, -0.412019282579422, -0.2636817991733551, -0.5084183216094971, -0.09294451773166656, -0.13322976231575012, -0.5703742504119873, 0.7031021118164062, -0.7872425317764282, -0.8905062675476074, 1.4930758476257324, 0.2362881898880005, -0.3471095561981201, 1.0463613271713257, 0.49181240797042847, -0.7939303517341614, 1.015120506286621, 0.3615555465221405, 0.5124908685684204, 0.12798988819122314, 0.8442392945289612, 0.15346786379814148, 0.38667869567871094, -0.8506510853767395, 0.012069668620824814, -0.10012715309858322, 0.03424627333879471, 0.2149549126625061, -0.755814790725708, -0.05347399413585663, 0.1785447597503662, 1.3430076837539673, -0.9930096864700317, 0.06379784643650055, -0.793060839176178, -0.04798663780093193, -1.0445538759231567, -0.4383530020713806, 0.8077442646026611, -0.24498966336250305, 0.1335248351097107, -0.845277726650238, -0.3250885605812073, -0.17273448407649994, 0.2081930935382843, 0.44423243403434753, -0.9654514789581299, -0.3318837881088257, 0.4504145383834839, -0.398846834897995, 0.4972535967826843, 0.5198189616203308, 0.46025633811950684, -0.7295866012573242, 0.20083659887313843, -0.5367283225059509, -0.348772794008255, 0.7102466225624084, 0.3268522322177887, 0.5320640206336975, 0.9093667268753052, -0.8167160153388977, 0.6181488633155823, 0.13076864182949066, 0.7170835733413696, -0.020722486078739166, 0.5149659514427185, -0.13113781809806824, -0.6046078205108643, 0.21770958602428436, -1.2462198734283447, 0.4216732382774353, -0.26097047328948975, 0.24842995405197144, -0.021642640233039856, 0.7954009771347046, -0.6137585043907166, -0.567706823348999, 0.34313464164733887, -0.36917319893836975, -0.8673781752586365, 1.0326176881790161, -0.33446332812309265, 0.04342752695083618, -0.8399974703788757, -0.14284175634384155, -0.2632787525653839, 0.6091396808624268, 0.6831803917884827, 0.7190802693367004, 0.7001480460166931, -0.7354268431663513, -0.061380334198474884, -0.05876938998699188, -0.8108158707618713, -0.500654399394989, -0.5490596294403076, -0.26906952261924744, -0.44195055961608887, -0.7754716277122498, -0.6596891283988953, 0.42992109060287476, -0.19303050637245178, 0.14804866909980774, 0.25696685910224915, 0.5987958908081055, 0.18723367154598236, 1.1661648750305176, 0.8709306120872498, -0.4764668345451355, 0.1546487957239151, -0.6585935354232788, 0.8569258451461792, -0.14055833220481873, -0.3944142460823059, -0.7507035732269287, -0.28625988960266113, -0.5871995091438293, -0.05700600892305374, 0.2637805640697479, 0.2109522670507431, -0.1859695464372635, 1.0820127725601196, 0.8555535078048706, 1.1266193389892578, -0.8300824165344238, -0.7425844073295593, 0.39606142044067383, 0.7428658604621887, 0.27563899755477905, -0.7563272714614868, 0.26232725381851196, -0.7452595829963684, 0.5276908278465271, 0.6696739792823792, -0.24236895143985748, -0.6461731195449829, 0.5519189834594727, -0.22389152646064758, -1.1386953592300415, 0.41630420088768005, 0.3790542185306549, -0.20093439519405365, -0.1959082931280136, -1.1995913982391357, 0.25582069158554077, 0.09192882478237152, -0.30772072076797485, -0.5571211576461792, 0.5456932187080383, -0.420360803604126, -0.06461764127016068, -0.1572573035955429, -0.8588362336158752, -1.2179938554763794, 0.7281590700149536, -0.42325496673583984, 0.42782965302467346, -0.32440924644470215, 0.3507530391216278, 0.3819501996040344, -0.5099730491638184, 0.39688143134117126, 0.558326244354248, -0.6543978452682495, -0.2639710605144501, 0.17449572682380676, 0.215509295463562, -0.2972070872783661, -0.7429357171058655, 1.3732517957687378, -0.3186032772064209, -0.5894370079040527, 0.8718301653862, 0.5502960681915283, -0.6107813119888306, -0.3971432149410248, 0.10378660261631012, -0.1480477899312973, 1.0325766801834106, 0.06787988543510437, 0.15031391382217407, 0.031089354306459427, -0.4962150454521179, -0.2544136345386505, -0.9543757438659668, 0.699049711227417, -0.5491633415222168, -0.9023526310920715, -0.8715752363204956, -1.0388462543487549, 0.006548549979925156, -0.01227407157421112, -0.157814159989357, 0.7301360964775085, 0.6681879758834839, -0.02503049373626709, -0.46518343687057495, -0.530913233757019, -0.2566179633140564, -0.5921066403388977, 0.293883740901947, 0.40661436319351196, 0.09291436523199081, 0.9708143472671509, 0.20831593871116638, -0.1521712690591812, -0.946415364742279, 0.6402827501296997, 0.32535260915756226, -0.3493097424507141, -1.2908680438995361, -1.0267748832702637, -0.6321037411689758, 0.21449163556098938, 0.2020157277584076, 0.4625770151615143, -0.5516478419303894, -0.30298614501953125, 0.38837137818336487, 0.26850426197052, 0.4649207890033722, -0.5663034915924072, -1.0900295972824097, 0.7617425322532654, 0.247260183095932, -0.6745837330818176, -0.5592234134674072, 0.24771395325660706, -0.26604002714157104, 0.2050878256559372, 0.03574401140213013, -0.29494401812553406, 0.0832085907459259, -0.613772451877594, 0.1313765048980713, -1.0078012943267822, -0.5306065082550049, -0.9369736909866333, 0.2965199947357178, -0.6858983635902405, 0.2999025583267212, 0.3505740761756897, -0.005129971541464329, -0.6029035449028015, -1.2889058589935303, -0.38756972551345825, 0.57569819688797, 0.07194413989782333, -0.1436847299337387, -0.3557591736316681, -0.4658212959766388, 1.3244789838790894, 0.2924806773662567, 0.5426639318466187, -0.2530680298805237, 0.06946352869272232, 0.37582650780677795, 0.9613381028175354, -0.4628946781158447, -0.47296643257141113, 0.47794899344444275, 0.03971210494637489, 0.09650310128927231, 0.2468501329421997, -0.04073253273963928, 0.6280357241630554, -0.3057553470134735, -0.038029879331588745, -0.027957074344158173, 0.6260483264923096, -0.36596915125846863, -0.3915450870990753, 0.38025200366973877, -1.1295781135559082, 0.52508544921875, 0.4265609085559845, 1.6218105554580688, 0.9197148084640503, 0.2814279794692993, -1.3836017847061157, -0.2535969018936157, -0.8652389049530029, -0.9707965850830078, 0.10138793289661407, -0.4389423727989197, -0.20177537202835083, 0.010586387477815151, 0.1658298820257187, 0.24322034418582916, 0.8229525089263916, 0.9073134660720825, 1.1869550943374634, 0.8401399254798889, -0.11375246942043304, -0.4946732819080353, 0.24889393150806427, 0.15572482347488403, -0.4742644131183624, 0.4613967835903168, -0.27863168716430664, -0.7604276537895203, -0.2733641564846039, -1.088485836982727, -0.8371622562408447, -0.6436958312988281, -0.18882958590984344, 1.3331583738327026, 0.3413035273551941, 0.9341609477996826, -0.536676824092865, -0.7844464182853699, -0.6080188155174255, 0.46729299426078796, -0.18441814184188843, 0.6205242276191711, 1.2383114099502563, 0.10663644224405289, 0.36215081810951233, -0.5765283703804016, -0.45774126052856445, 0.0007195696234703064, -0.9157689213752747, 1.0444362163543701, -0.14899912476539612, -0.1944294273853302, 0.5510694980621338, 1.0451042652130127, -0.8523048162460327, -0.9578858613967896, 0.3504002094268799, -0.47824859619140625, -0.29733261466026306, -1.1921615600585938, -0.6174532175064087, -0.8735268712043762, 0.024961352348327637, 0.3518464267253876, 1.9243663549423218, -0.027646686881780624, 0.25747111439704895, -0.27379748225212097, 0.6428434252738953, -1.3239418268203735, 0.07271929085254669, 0.7038095593452454, 0.3590875566005707, -0.3302423357963562, -0.8499006628990173, -0.22478297352790833, 0.3172883689403534, 0.04042001813650131, 0.3513835370540619, -0.5818154811859131, 0.8415183424949646, 0.9978369474411011, -0.08003076165914536, 0.6530058979988098, -0.13784688711166382, 0.8687486052513123, -0.43848589062690735, -0.2936623990535736, -0.9274079203605652, -0.14556147158145905, 0.05759797245264053, -0.34082669019699097, 0.5136773586273193, 0.10539525747299194, 0.61822909116745, 0.2008548378944397, 0.9273242950439453, -0.11269134283065796, -1.0771148204803467, -0.5847330093383789, -1.2518725395202637, -0.6174278855323792, -0.9030261039733887, 0.4093974530696869, -0.4310470223426819, 0.4589693546295166, -0.30038201808929443, -1.539288878440857, -0.14327344298362732, 0.5667564868927002, -0.4856840968132019, 0.6696324944496155, -0.7159135341644287, 0.8249067068099976, -1.146281361579895, -0.4400075078010559, -0.6842167973518372, -0.04269525781273842, -0.2751290202140808, 0.5118878483772278, 0.17874473333358765, 0.17147046327590942, -0.46736469864845276, 1.644307017326355, -0.1609041839838028, 1.0409983396530151, 0.3638838231563568, -0.7553738355636597, 0.12639859318733215, 0.2169317752122879, 0.6490988731384277, 0.750028133392334, 0.4989416301250458, 0.036799874156713486, 0.25811904668807983, 0.05029056966304779, -0.2895582318305969, -0.42389777302742004, -0.13982412219047546, 1.0108144283294678, 0.22353973984718323, -0.3637789189815521, -0.23185639083385468, 0.0342877134680748, 0.025035250931978226, 0.02613036334514618, 0.06535490602254868, 0.2899759113788605, 0.9134537577629089, -0.5988917946815491, 0.041839197278022766, 0.4883764684200287, -0.04468386992812157, 0.27665597200393677, -0.1497846245765686, -0.015755312517285347, 0.17805910110473633, -0.058781638741493225, -0.07614783942699432, -0.1674165427684784, 0.44359418749809265, -0.4182249903678894, 0.8374955654144287, 0.6997677683830261, -0.2898101210594177, 0.5621848106384277, -0.9392961263656616, -0.8812183141708374, -1.0499004125595093, 0.40011024475097656, -0.23955637216567993, 0.226935476064682, 0.13018225133419037, 0.33182069659233093, 0.42305222153663635, -0.4642927050590515, -0.719851553440094, -0.016144562512636185, -0.4804210662841797, -0.8657574653625488, -0.5974353551864624, -0.05039456486701965, -0.8258897662162781, 0.19818873703479767, -1.0376414060592651, 0.5329832434654236, -0.8777660131454468, 0.2067168951034546, 0.11420206725597382, 0.22739511728286743, 0.20605455338954926, 0.5030332207679749, -1.0579321384429932, 0.5603997707366943, 0.25539344549179077, 0.3828064799308777, -0.8259814977645874, -0.031228939071297646, 0.5365481376647949, -0.006161600351333618, 0.10935216397047043, 0.17645081877708435, -0.42985737323760986, 0.3421502411365509, -0.2895123064517975, -0.7322322130203247, 0.6268496513366699, 1.117628574371338, -0.17349699139595032, -0.03078080713748932, -0.12357290089130402, 0.7067403793334961, -0.4216356873512268, 0.8381470441818237, -0.262001097202301, -0.30678847432136536, -0.5695270299911499, 1.3890663385391235, 0.4600595235824585, -0.6264137625694275, 0.787406861782074, 0.8638224005699158, -0.24741600453853607, -0.31445059180259705, -0.20179904997348785, -0.34850141406059265, 0.7167356610298157, 1.2073473930358887, 0.9465433955192566, -0.10660585761070251, 0.1268172562122345, 0.6309024691581726, 0.16118451952934265, -0.08246641606092453, 0.7685756087303162, 0.3560386300086975, -0.25651296973228455, -0.1337348222732544, -0.13870243728160858, 0.877117395401001, 0.2316213995218277, 0.05115839093923569, -0.38629335165023804, -0.45333975553512573, -0.6902398467063904, 0.7321274876594543, -0.4762885868549347, 1.0209708213806152, -0.2800058424472809, -0.19972670078277588, -0.95366370677948, -0.11276768147945404, 0.2132456749677658, 0.08982142806053162, 0.36845311522483826, 0.38905203342437744, 0.025252297520637512, 0.16402839124202728, -0.509046733379364, 0.3563801050186157, -0.7294796109199524, 0.3671875, -0.5227496027946472, 0.3949524760246277, 0.07912282645702362, 0.02601827122271061, -0.5645018219947815, 0.2949836850166321, -0.45406848192214966, 1.6215049028396606, -0.2937077283859253, 0.4864468574523926, 0.555425226688385, -0.4138256013393402, -0.5542564988136292, -1.2297987937927246, 1.065573811531067, -0.62039715051651, -0.7084008455276489, 0.17753303050994873, -0.4522542357444763, -0.5471095442771912, 0.5439791679382324, -0.3399167060852051, 0.04414623975753784, 0.5700980424880981, 1.3415675163269043, -0.08569534122943878, 0.7205119729042053, 0.7326139211654663, -0.8530216217041016, -0.24033337831497192, -0.3915342092514038, 0.03238202631473541, 0.008244555443525314, -0.07724817097187042, -0.10695965588092804, 0.24077026546001434, -1.1556745767593384, -0.10130098462104797, 0.029168274253606796, 0.4935583174228668, -0.32734444737434387, -0.7721831202507019, -0.03843364119529724, 0.446859747171402, -0.21963638067245483, 0.7924382090568542, 0.7836179733276367, 1.4833728075027466, -0.6509734988212585, 0.015557666309177876, -0.36821305751800537, -0.7935572266578674, -0.012227166444063187, -1.0894500017166138, 1.1111546754837036, -0.5887078642845154, -1.0850474834442139, -0.15241171419620514, -0.7655759453773499, -0.748809814453125, 0.16946528851985931, -0.3902938961982727, -1.2257194519042969, 0.6254821419715881, 0.672921359539032, 0.11853116750717163, 0.18351860344409943, -0.6688340306282043, 0.36599963903427124, 0.6360313892364502, 0.9352466464042664, 0.3773680329322815, 1.162436604499817, 0.38806575536727905, 0.09426674991846085, 1.1416282653808594, -1.1023900508880615, 1.0768821239471436, 0.07263636589050293, -0.6930216550827026, 0.04273437708616257, 0.12486567348241806, -0.4361974596977234, -0.7063398957252502, 0.09575432538986206, 0.5466373562812805, -0.17924055457115173, -0.18791180849075317, -1.5774273872375488, -1.0810877084732056, -1.021265983581543, -0.04405895993113518, -1.1178677082061768, 1.0327060222625732, -0.0512031689286232, -0.678183913230896, 0.032891299575567245, -0.8254417181015015, 3.2439112663269043, 1.0333398580551147, 0.6323299407958984, -0.2285783886909485, 0.4947689473628998, 1.0012896060943604, 0.7586359977722168, -0.3360176980495453, 0.352227121591568, -0.5108520984649658, 0.8171559572219849, 0.20486506819725037, -0.06749607622623444, 0.5344784259796143, -0.3113206923007965, 1.0364954471588135, -0.920412540435791, -0.08400227129459381, 0.5725063681602478, -0.5435321927070618, -0.896837055683136, 0.281009316444397, 0.07582610100507736, 0.6225445866584778, -0.22964845597743988, 0.2084210216999054, -0.07551329582929611, -1.0541064739227295, -0.1264764666557312, -0.7853209972381592, 0.7383602261543274, -0.6023451089859009, -0.032430730760097504, -0.44870761036872864, 0.49172455072402954, -0.07703567296266556, -0.6555184125900269, -0.5073776841163635, 0.4169193506240845, 0.47805583477020264, 0.1129460334777832, 0.4881584048271179, 0.3532664477825165, -0.4592067003250122, 0.5008076429367065, 0.1311531662940979, -0.7457735538482666, -0.19607385993003845, -0.046233899891376495, -1.1640852689743042, 0.5788649320602417, -0.06549303978681564, 0.5762708783149719, -0.06636162102222443, -0.4171205461025238, -0.06382133811712265, -0.21386267244815826, -0.8837346434593201, -0.2996496856212616, 0.12226789444684982, 0.46490904688835144, -0.4693593382835388, 0.1991998851299286, 0.034368082880973816, -0.6166280508041382, 0.5714478492736816, -0.13200221955776215, 0.15212565660476685, -1.0689051151275635, -0.2824689447879791, -0.3038617968559265, -0.8054931163787842, -0.730923593044281, -0.6443650722503662, -0.13492053747177124, 0.26193612813949585, -1.0789005756378174, 0.10571663081645966, -0.09420796483755112, -0.7039891481399536, -0.021592378616333008, -0.4875580072402954, 0.3090980052947998, 0.13581788539886475, 0.07342174649238586, 0.6265999674797058, -0.06512981653213501, -0.35303691029548645, -0.22171305119991302, 1.301042914390564, 1.387018084526062, 0.36730509996414185, 0.2050064504146576, -0.5011556148529053, -0.1768965721130371]}\n"
     ]
    }
   ],
   "source": [
    "# # 임베딩 예시\n",
    "# import ollama\n",
    "# exmaple_embeddings = ollama.embeddings(\n",
    "#   prompt=df['summ_context'][0],\n",
    "#   model='mxbai-embed-large',\n",
    "# )\n",
    "# print(exmaple_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 함수화\n",
    "import ollama\n",
    "def get_embedding(prompt, model=\"mxbai-embed-large\"):\n",
    "    response = ollama.embeddings(prompt=prompt, model=model)\n",
    "    return response[\"embedding\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon_mys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
