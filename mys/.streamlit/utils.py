# ìŠ¤íŠ¸ë¦¼ë¦¿
import streamlit as st

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ 2ê°€ì§€
from langchain_community.chat_models import ChatOllama
from langchain_ollama.llms import OllamaLLM

# í”„ë¡¬í”„íŠ¸ ê´€ë ¨
from langchain_core.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ëŒ€í™” ê¸°ë¡ ê´€ë ¨
from langchain_core.messages import ChatMessage
from langchain_core.prompts import MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ ê´€ë ¨
from langchain_core.callbacks.base import BaseCallbackHandler

import os

def start_streamlit(page_title="MoonYoungSik"): 
    # í˜ì´ì§€ ê¸°ë³¸ ì…‹íŒ…
    st.set_page_config(page_title=page_title,  page_icon="ğŸ’¬")
    st.title("ğŸ’¬"+ page_title+" Test")
    # ì‚¬ì´ë“œë°” ìƒì„±
    with st.sidebar:
        st.session_state.session_id = st.text_input("ì‚¬ìš©ìëª…", value="ë¬¸ì˜ì‹")
        clear_btn = st.button("ëŒ€í™”ê¸°ë¡ ì´ˆê¸°í™”")
        if clear_btn:
            # í•´ë‹¹ session_idì— ëŒ€í•œ ëŒ€í™” ê¸°ë¡ë§Œ ì´ˆê¸°í™”
            if st.session_state.session_id in st.session_state["store"]:
                del st.session_state["store"][st.session_state.session_id]
            st.session_state["messages"] = []  # ì „ì²´ ë©”ì‹œì§€ ì´ˆê¸°í™”
            st.rerun  # ë³€ê²½ ì‚¬í•­ ë°˜ì˜ì„ ìœ„í•´ rerun í˜¸ì¶œ

def service_init(): 
    # ì„¸ì…˜ ì´ˆê¸°í™”
    if "messages" not in st.session_state: # ëŒ€í™” ê¸°ë¡ ì¶œë ¥ìš©
        st.session_state["messages"] = []

    if "store" not in st.session_state:  # ëŒ€í™” ê¸°ë¡ ì°¸ê³ ìš©
        st.session_state["store"] = dict()

def print_messages(): 
    # ì±—ë´‡ì€ ì§ˆë¬¸ì„ í•œ ë²ˆ í•˜ê³  ë‹µë³€ì„ í• ë•Œë§ˆë‹¤ ìƒˆë¡œê³ ì¹¨ì´ ì¼ì–´ë‚˜ëŠ” íŠ¹ì„±ì´ ìˆë‹¤.
    # ì´ì „ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ messages ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë‹¤.
    # ì„¸ì…˜ì´ ìˆê³  ì„¸ì…˜ì— ë©”ì„¸ì§€ê°€ ìˆë‹¤ë©´ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì¶œë ¥
    if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
        for chat_message in st.session_state["messages"]:
            st.chat_message(chat_message.role).write(chat_message.content)

def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    # ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids] # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë³€í™˜

class StreamHandler(BaseCallbackHandler): 
    # AI ë‹µë³€ ìƒì„± ë°©ì‹ ë°”ê¾¸ê¸° : ì „ì²´ í† í° ìƒì„±ë˜ê³  ë‚˜ì˜¤ëŠ” ë°©ì‹ -> í† í° ìƒì„±ë  ë•Œë§ˆë‹¤ ì¶œë ¥ ë°©ì‹
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
    def on_llm_new_token(self, token:str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# # llm = ChatOllama(model= 'MoonYoungSik') # ì—¬ëŸ¬ê°€ì§€ ì†ì„± ê°’ì´ ë‚˜ì˜´.
# # llm = OllamaLLM(model= 'MoonYoungSik') # ì¶œë ¥ê°’ì´ ë‹µë³€ë§Œ ë‚˜ì˜´.
# # llm = ChatOllama(model= 'gemma2') # ollama ìì²´ gemma2-9b ì…ë‹ˆë‹¤. ì§ˆì˜ ì‘ë‹µ ìˆ˜ì¤€ì´ ë”°ë¡œ ì„¤ì •í•œê²ƒ ì—†ì´ SYSTEM ì„¤ì •ì´ ì˜ë˜ì–´ìˆëŠ” ë“¯ í•¨.
output_parser = StrOutputParser()
def input_output():
    # ìœ ì €ì˜ ì…ë ¥ê³¼ ì¶œë ¥ê¹Œì§€ ì „ë°˜ì ì¸ ì½”ë“œ
    if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."):
        st.chat_message("user").write(f"{user_input}") # ìœ ì €ì˜ ë©”ì„¸ì§€ë¥¼ ì¶œë ¥
        st.session_state["messages"].append(ChatMessage(role="user", content=user_input)) # ìœ ì €ì˜ ë©”ì„¸ì§€ë¥¼ messages ë¦¬ìŠ¤íŠ¸ì— ë‹´ëŠ”ë‹¤. print_messages() í•¨ìˆ˜ë¥¼ ë³´ë©´ ì´ìœ ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.

        # AIì˜ ë‹µë³€
        with st.chat_message("assistant"): 
            # ìŠ¤íŠ¸ë¦¼ í•¸ë“¤ëŸ¬ì™€ í•¨ê»˜ AI ëª¨ë¸ ì„¤ì •
            stream_handler = StreamHandler(st.empty())
            # llm = ChatOllama(model= 'gemma2', streaming=True, callbacks=[stream_handler])
            llm = ChatOllama(model='gemma2', streaming=True, callbacks=[stream_handler], max_input_tokens=None, max_output_tokens=None)

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ì´ ì‹œìŠ¤í…œì€ í•œêµ­ì¸ '{username}' ë‹˜ì„ ëŒ€ìƒìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  {ability} ë¶„ì„ì„ ì˜í•˜ê³  íˆ¬ì ì¡°ì–¸ë„ ì˜í•©ë‹ˆë‹¤."),
                MessagesPlaceholder(variable_name="history"),
                ("user", "{question}"),
            ])

            # í”„ë¡¬í”„íŠ¸ì™€ AI ëª¨ë¸ì„ ì—°ê²°
            chain = prompt | llm | output_parser

            # ë©”ì„¸ì§€ ê¸°ë¡ì„ í™œìš©í•œ ì²´ì¸ ê°ì²´ ìƒì„±
            chain_with_memory = RunnableWithMessageHistory( 
                chain, # ì²´ì¸
                get_session_history, # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
                history_messages_key="history", # ê¸°ë¡ ë©”ì„¸ì§€ì˜ key
                input_messages_key="question", # ì‚¬ìš©ì ì§ˆë¬¸ì˜ key
            )

            # AI ì‘ë‹µ ìƒì„±
            response = chain_with_memory.invoke(
                {"ability": "ì£¼ì‹", "username": st.session_state.session_id ,"question": user_input},
                config={"configurable": {"session_id": st.session_state.session_id}}
            )
            # ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
            st.session_state["messages"].append(ChatMessage(role="assistant", content=response))