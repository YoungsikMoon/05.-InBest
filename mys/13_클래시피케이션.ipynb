{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 한글 깨짐 방지\n",
    "# https://velog.io/@redgreen/Linux-linux%EC%97%90%EC%84%9C-Matplotlib-%ED%95%9C%EA%B8%80%ED%8F%B0%ED%8A%B8-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0\n",
    "\n",
    "import matplotlib.font_manager\n",
    "font_list = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "[matplotlib.font_manager.FontProperties(fname=font).get_name() for font in font_list if 'Nanum' in font]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='NanumGothicCoding')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realUp</th>\n",
       "      <th>summ_context_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      realUp  summ_context_len\n",
       "2699       0               251\n",
       "2700       1               224\n",
       "2701       0               174\n",
       "2702       0               169\n",
       "2703       0               166"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # CSV 파일 읽기\n",
    "# summ_df = pd.read_csv('/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3.csv')\n",
    "\n",
    "# columns = ['realUp', 'stock', 'title', 'summ_context_len', 'ori_context_len', 'diff_len']\n",
    "# temp_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# for i in range(len(summ_df)):\n",
    "#     temp_df.loc[i, 'realUp'] = summ_df.loc[i, 'realUp']\n",
    "#     temp_df.loc[i, 'stock'] = summ_df.loc[i, 'stock']\n",
    "#     temp_df.loc[i, 'title'] = summ_df.loc[i, 'title']\n",
    "#     temp_df.loc[i, 'summ_context_len'] = len(summ_df.loc[i, 'summ_context'])\n",
    "#     temp_df.loc[i, 'ori_context_len'] = len(summ_df.loc[i, 'ori_context'])\n",
    "#     temp_df.loc[i, 'diff_len'] =  temp_df.loc[i, 'summ_context_len'] - temp_df.loc[i, 'ori_context_len']\n",
    "# temp_df.tail()\n",
    "\n",
    "\n",
    "# 개선된 코드\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "summ_df = pd.read_csv('/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-1.csv')\n",
    "\n",
    "# 필요한 열만 선택하여 새로운 데이터프레임 생성\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "# 각 열에 대해 apply 사용\n",
    "temp_df['realUp'] = summ_df['realUp']\n",
    "# temp_df['stock'] = summ_df['stock']\n",
    "# temp_df['title'] = summ_df['title']\n",
    "temp_df['summ_context_len'] = summ_df['summ_context'].apply(len)\n",
    "# temp_df['ori_context_len'] = summ_df['ori_context'].apply(len)\n",
    "\n",
    "# diff_len 계산\n",
    "# temp_df['diff_len'] = temp_df['summ_context_len'] - temp_df['ori_context_len']\n",
    "\n",
    "# 결과 확인\n",
    "display(temp_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 원본 기사 글자 수 분포\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# summ_df['ori_context'].str.len().hist(bins=30, color='pink')  # 막대 색상 지정\n",
    "# plt.title('원본 기사 글자 수 분포')\n",
    "# plt.xlabel('글자 수')\n",
    "# plt.ylabel('빈도')\n",
    "# plt.show()\n",
    "\n",
    "# 요약 기사 글자 수 분포\n",
    "plt.figure(figsize=(8, 4))\n",
    "summ_df['summ_context'].str.len().hist(bins=30, color='lightgreen')  # 막대 색상 지정\n",
    "plt.title('요약 기사의 글자 수 분포')\n",
    "plt.xlabel('글자 수')\n",
    "plt.ylabel('빈도')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 라벨 분포\n",
    "value_counts = summ_df['realUp'].value_counts()\n",
    "total_count = value_counts.sum()\n",
    "plt.figure(figsize=(8, 4))\n",
    "value_counts.plot(kind='bar', color=['red', 'blue'])  # 막대 색상 지정\n",
    "plt.title('라벨 분포')\n",
    "plt.xlabel('realUp')\n",
    "plt.ylabel('Count')\n",
    "# 총 카운트 수 표시\n",
    "plt.text(0, 1.2, f'Total Count: {total_count}', transform=plt.gca().transAxes, ha='center', va='top')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "realUp\n",
       "0    1661\n",
       "1    1043\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ_df['realUp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "75\n",
      "92\n",
      "89\n",
      "40\n",
      "93\n",
      "99\n",
      "84\n",
      "93\n",
      "59\n",
      "79\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(temp_df)):\n",
    "    if temp_df.loc[i, 'summ_context_len'] < 100 : # 뉴스 내용이 너무 적어서 학습에 방해될 것 3개\n",
    "        print(temp_df.loc[i, 'summ_context_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      realUp                                       summ_context\n",
      "104        0  코스닥지수가 외국인 순매수에 힘입어 일 연속 상승세를 이어가며 선을 회복했습니다 의...\n",
      "151        0  정부의 북한 조림 사업 추진 계획이 기대되면서 조림 관련 종목이 초반 상승세를 보였...\n",
      "210        0  한국 코스피지수는 외국인 순매수로 인해 상승하여 로 마감하였습니다 삼성전자 하이닉스...\n",
      "368        0  휴네시온 이 상한가를 기록 중이며 현재 원에 거래되고 있습니다 최근 거래량이 급증했...\n",
      "562        0           네오오토 장 시작부터 높은 상승세 보여 오전 시 분 현재 전일 대비 증가\n",
      "739        0  케이엠제약이 시작부터 강한 상승세를 보이며 장중 최고 까지 올랐다가 현재는 상승한 ...\n",
      "1141       1  한국거래소 코스닥시장본부가 유니온커뮤니티의 주가 급등에 대해 조회공시를 요구하고 있...\n",
      "1143       1  휴맥스는 연결 기준으로 올해 분기 영업이익이 지난해 동기 대비 감소했다고 공시했습니...\n",
      "1189       0  오늘 코스피 지수는 외국인 매도에도 불구하고 개인이 꾸준히 순매수하면서 상승해 에 ...\n",
      "1396       1  한국 코스닥지수 상승 외국인 및 기관투자자 동반 매수세 오락 문화 관련주가 강세 키...\n",
      "1707       0  주요 내용 요약 코스닥 지수가 거래일 만에 하락하여 개인 투자자의 순매수로 간신히 ...\n",
      "2009       0  일 코스피 지수 전 거래일 대비 상승하며 로 마감 외국인 및 기관이 매수를 이어가며...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "temp_df['realUp'] = summ_df[['realUp']]\n",
    "temp_df['summ_context'] = summ_df[['summ_context']]\n",
    "\n",
    "# summ_context의 길이가 00 미만인 행 필터링\n",
    "filtered_df = temp_df[temp_df['summ_context'].str.len() <100]\n",
    "\n",
    "# 결과 출력\n",
    "print(filtered_df) #60 미만 부터 이유가 없음\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "filtered_df.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   realUp                                       summ_context\n",
      "0       1  국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승...\n",
      "1       1  우리기술투자 가 가상화폐 거래소인 업비트를 운영하는 두나무 지분 가치가 주목받고 있...\n",
      "2       1  뉴프라이드 는 미국 대마초 판매 합법화에 따라 코스닥 시장에서 상승세를 보이고 있습...\n",
      "3       0  요약하자면 코스피가 연속 상승세를 이어가며 사상 최고치인 선을 향해 가고 있고 코스...\n",
      "4       1  코스닥지수는 바이오 제약주들의 차익 실현 매물로 인해 하락세를 보였습니다 기관은 대...\n"
     ]
    }
   ],
   "source": [
    "# summ_context의 길이가 60 미만인 행 삭제\n",
    "temp_df = temp_df[temp_df['summ_context'].str.len() >= 100]\n",
    "print(temp_df.head())\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      realUp                                       summ_context\n",
      "0          1  국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승...\n",
      "1          1  우리기술투자 가 가상화폐 거래소인 업비트를 운영하는 두나무 지분 가치가 주목받고 있...\n",
      "2          1  뉴프라이드 는 미국 대마초 판매 합법화에 따라 코스닥 시장에서 상승세를 보이고 있습...\n",
      "3          0  요약하자면 코스피가 연속 상승세를 이어가며 사상 최고치인 선을 향해 가고 있고 코스...\n",
      "4          1  코스닥지수는 바이오 제약주들의 차익 실현 매물로 인해 하락세를 보였습니다 기관은 대...\n",
      "...      ...                                                ...\n",
      "2687       0  해당 기사는 코스의 지수가 미국 금리 하락 기대가 줄어들고 이른 폭염으로 인해 빙과...\n",
      "2688       1  코스닥 지수는 외국인 매도로 인해 거래일 만에 하락 마감했습니다 코스닥은 오후 한때...\n",
      "2689       0  코스닥 지수가 상승세를 보이며 전거래일 대비 오른 로 거래 중입니다 나스닥발 훈풍으...\n",
      "2690       0  라씨로는 기반 주식 분석 정보를 제공하는 전자신문 증권 정보 애플리케이션입니다 플레...\n",
      "2691       0  증권은 삼양식품의 주요 고객사인 식품소재 기업 에스앤디 가 불닭볶음면 열풍으로 실적...\n",
      "\n",
      "[2692 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "temp_df = pd.read_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-2.csv')\n",
    "\n",
    "# 한글만 남기고 나머지 제거하는 함수 정의\n",
    "def keep_hangul(text):\n",
    "    return ' '.join(re.findall(r'[가-힣]+', text))\n",
    "\n",
    "# summ_context 열에 한글만 남기기\n",
    "temp_df['summ_context'] = temp_df['summ_context'].apply(keep_hangul)\n",
    "\n",
    "# 결과 출력\n",
    "print(temp_df)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: 안녕하세요 오늘의 뉴스 요약입니다.\n",
      "KoGPT2 토큰화된 결과: ['▁안녕', '하', '세', '요', '▁오늘의', '▁뉴스', '▁요약', '입니다.']\n",
      "KoBERT 토큰화된 결과: ['안녕하세요', '오늘의', '뉴스', '요약', '##입니다', '.']\n",
      "Okt_nouns 토큰화된 결과: ['오늘', '뉴스', '요약']\n",
      "Okt_morphs 토큰화된 결과: ['안녕하세요', '오늘', '의', '뉴스', '요약', '입니다', '.']\n",
      "KoGPT2 토큰 ID: [25906, 8702, 7801, 8084, 34461, 26506, 17669, 21154]\n",
      "KoBERT 토큰 ID: [16453, 17622, 21585, 26539, 13992, 2016]\n",
      "KoGPT2 디코딩된 텍스트: 안녕하세요 오늘의 뉴스 요약입니다.\n",
      "KoBERT 디코딩된 텍스트: 안녕하세요 오늘의 뉴스 요약입니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast # KoGPT2\n",
    "from transformers import BertTokenizer # KoBERT\n",
    "from konlpy.tag import Okt # okt\n",
    "\n",
    "\n",
    "# KoGPT 모델에 맞는 토크나이저 로드\n",
    "tokenizer_KoGPT = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "# KoBERT 모델에 맞는 토크나이저 로드\n",
    "tokenizer_KoBERT = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n",
    "# OKT\n",
    "okt = Okt()\n",
    "\n",
    "# 토큰화할 텍스트\n",
    "text = \"안녕하세요 오늘의 뉴스 요약입니다.\"\n",
    "\n",
    "# 텍스트를 토큰화\n",
    "tokens_KoGPT = tokenizer_KoGPT.tokenize(text)\n",
    "tokens_KoBERT =tokenizer_KoBERT.tokenize(text)\n",
    "\n",
    "# 토큰 ID 변환\n",
    "token_ids_KoGPT = tokenizer_KoGPT.convert_tokens_to_ids(tokens_KoGPT)\n",
    "token_ids_KoBERT =tokenizer_KoBERT.convert_tokens_to_ids(tokens_KoBERT)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"원본 텍스트:\", text)\n",
    "print(\"KoGPT2 토큰화된 결과:\", tokens_KoGPT)\n",
    "print(\"KoBERT 토큰화된 결과:\", tokens_KoBERT)\n",
    "print(\"Okt_nouns 토큰화된 결과:\", okt.nouns(text))\n",
    "print(\"Okt_morphs 토큰화된 결과:\", okt.morphs(text))\n",
    "print(\"KoGPT2 토큰 ID:\", token_ids_KoGPT)\n",
    "print(\"KoBERT 토큰 ID:\", token_ids_KoBERT)\n",
    "\n",
    "# 토큰 ID를 다시 텍스트로 변환\n",
    "decoded_text_KoGPT = tokenizer_KoGPT.decode(token_ids_KoGPT)\n",
    "decoded_text_KoBERT = tokenizer_KoBERT.decode(token_ids_KoBERT)\n",
    "print(\"KoGPT2 디코딩된 텍스트:\", decoded_text_KoGPT)\n",
    "print(\"KoBERT 디코딩된 텍스트:\", decoded_text_KoBERT)\n",
    "\n",
    "# 원본 텍스트: 안녕하세요, 오늘의 뉴스 요약입니다.\n",
    "# KoGPT2 토큰화된 결과: ['▁안녕', '하', '세', '요,', '▁오늘의', '▁뉴스', '▁요약', '입니다.']\n",
    "# KoBERT 토큰화된 결과: ['안녕하세요', ',', '오늘의', '뉴스', '요약', '##입니다', '.']\n",
    "# Okt_nouns 토큰화된 결과: ['오늘', '뉴스', '요약']\n",
    "# Okt_morphs 토큰화된 결과: ['안녕하세요', ',', '오늘', '의', '뉴스', '요약', '입니다', '.']\n",
    "# KoGPT2 토큰 ID: [25906, 8702, 7801, 13704, 34461, 26506, 17669, 21154]\n",
    "# KoBERT 토큰 ID: [16453, 2014, 17622, 21585, 26539, 13992, 2016]\n",
    "# KoGPT2 디코딩된 텍스트: 안녕하세요, 오늘의 뉴스 요약입니다.\n",
    "# KoBERT 디코딩된 텍스트: 안녕하세요, 오늘의 뉴스 요약입니다.\n",
    "\n",
    "# 토큰화는 Okt_nouns가 좋을것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okt nouns 처리한 파일이 'naver_news_origin_duplicates_summ3-4.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter  # Counter 클래스 임포트\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 토크나이저 로드\n",
    "okt = Okt()\n",
    "\n",
    "# summ_context 열에 Okt 토크나이저 적용 및 1글자 이하의 토큰 삭제\n",
    "temp_df['okt_tokenized'] = temp_df['summ_context'].apply(\n",
    "    lambda x: [word for word in okt.nouns(x) if len(word) > 1]  # 1글자 이하 토큰 필터링\n",
    ")\n",
    "\n",
    "# 토큰화 데이터프레임 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-4.csv')\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"okt nouns 처리한 파일이 'naver_news_origin_duplicates_summ3-4.csv'로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모든 토큰을 하나의 리스트로 결합\n",
    "# all_tokens = [token for sublist in temp_df['okt_tokenized'] for token in sublist]\n",
    "\n",
    "# # 단어 빈도수 계산\n",
    "# word_counts = Counter(all_tokens)\n",
    "\n",
    "# # 빈도수 내림차순 정렬\n",
    "# sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # 결과를 데이터프레임으로 변환\n",
    "# word_freq_df = pd.DataFrame(sorted_word_counts, columns=['단어', '빈도수'])\n",
    "\n",
    "# # 결과를 CSV 파일로 저장\n",
    "# word_freq_df.to_csv('word_frequency.csv', index=False)\n",
    "\n",
    "# # 저장 완료 메시지 출력\n",
    "# print(\"단어 빈도수 파일이 'word_frequency.csv'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (희소문제, 단어 순서 무시, 문맥 정보 부족, 문서 길이 편향, 어휘 확장성 부족) 사용 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 데이터프레임:\n",
      "       가가        가격   가결   가공  가공업   가구  가까이   가능       가능성   가닥  ...   희소  \\\n",
      "0     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "1     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.100542  0.0  ...  0.0   \n",
      "2     0.0  0.113091  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "3     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "4     0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "...   ...       ...  ...  ...  ...  ...  ...  ...       ...  ...  ...  ...   \n",
      "2687  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2688  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2689  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2690  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "2691  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  ...  0.0   \n",
      "\n",
      "      희소식  희토류   히터   히트  히트텍  힌남노   힐링  힘겨루기   힘스  \n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...   ...  ...  \n",
      "2687  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2688  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2689  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2690  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "2691  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[2692 rows x 7945 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast  # 문자열을 리스트로 변환하기 위한 모듈\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 문자열 형태의 리스트를 실제 리스트로 변환\n",
    "temp_df['okt_tokenized'] = temp_df['okt_tokenized'].apply(ast.literal_eval)\n",
    "\n",
    "# 토큰 리스트를 문자열로 변환\n",
    "temp_df['text'] = temp_df['okt_tokenized'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(temp_df['text'])\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"TF-IDF 데이터프레임:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# TF-IDF 벡터화기 저장\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "# TF-IDF 매트릭스 저장 (필요한 경우)\n",
    "tfidf_df.to_csv('tfidf_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 데이터에 대한 TF-IDF 데이터프레임:\n",
      "    가가   가격   가결   가공  가공업   가구  가까이   가능  가능성   가닥  ...   희소  희소식  희토류   히터  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "    히트  히트텍  힌남노   힐링  힘겨루기   힘스  \n",
      "0  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[2 rows x 7945 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 새로운 데이터 예시\n",
    "new_data = [\"안녕하세요 나는 문영식 입니다 벡터화 테스트 입니다\", \"어쩔탱 저쩔탱 쿠쿠르삥뽕 안물 안궁\"]\n",
    "\n",
    "# TF-IDF 벡터화기 로드\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_vectorizer = pickle.load(f)\n",
    "\n",
    "# 새로운 데이터를 DataFrame으로 변환\n",
    "new_df = pd.DataFrame(new_data, columns=['text'])\n",
    "\n",
    "# 새로운 데이터에 대해 TF-IDF 벡터화\n",
    "new_tfidf_matrix = loaded_vectorizer.transform(new_df['text'])\n",
    "\n",
    "# TF-IDF 결과를 데이터프레임으로 변환\n",
    "new_tfidf_df = pd.DataFrame(new_tfidf_matrix.toarray(), columns=loaded_vectorizer.get_feature_names_out())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"새로운 데이터에 대한 TF-IDF 데이터프레임:\")\n",
    "print(new_tfidf_df)\n",
    "\n",
    "# 필요시 새로운 TF-IDF 결과를 CSV 파일로 저장\n",
    "new_tfidf_df.to_csv('new_tfidf_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ko_BERT로 임베딩해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과:\n",
      "{'input_ids': tensor([[    2,  8982,  3933,  9415, 14866,  8535,  9246, 11608,  5138,  9154,\n",
      "          9389,  5167,  2953, 10464,  5105, 14295, 11539, 10999,  3599,  5817,\n",
      "         10695, 10026, 14589,  9594, 19509,  5284, 11329,  5816,  8667,  9767,\n",
      "          3683,  8932,  8961,  9445,  1998,  5238, 13322,  8513,  9000,  9566,\n",
      "          5310,  5138,  3683, 10741,  1920, 19501, 17821,  9594,  3824,  5345,\n",
      "          9396,  5351,  5284, 17048, 18499, 15905,  8454,  1932, 11000, 17373,\n",
      "          5072, 10983, 10464,  5019,  3322,  5550, 10460, 12711,  9255,  8538,\n",
      "         10624,  8465,  3682, 17996,  1932, 13658,  5067, 11953,  9602, 12424,\n",
      "          5051,  8486, 14866,  8535, 11608,  5008,  9751,  8455,  9594,     3,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    2,  8472, 10984, 12148,  1920, 17106, 19319, 10253,  5208,  5096,\n",
      "          3576,  5444, 11385, 16135,  2500, 17566, 16429, 14932, 13279,  9177,\n",
      "          9074,  2088,  5506,  8455,  9594,  3819,  5147,  5655,  3707,  5166,\n",
      "         12148, 15673,  9536,  9483,  3576,  5444, 18532,  3749, 10253, 19285,\n",
      "          5022,  3546, 18087,  8452,  9054,  5214,  5678, 16412, 11480,  5022,\n",
      "          3546,  3566,  5127,  5037,  8804,  4137,  5132,  9066,  8538, 15363,\n",
      "          5105, 16429, 11922,  5035, 12344,  8758, 10149, 10504,  5088,  8804,\n",
      "         10302, 15123, 17106, 19319,  9246,  5019,  9973,  9389,  5178, 10014,\n",
      "         10384, 11606, 14196,  5072, 11608, 10247,     3,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    2,  2361, 16443, 12026,  2367,  8523, 16959,  5817,  9779, 13430,\n",
      "         15318,  8758, 11329,  5816, 18642, 10695, 10026, 14589,  9594,  2361,\n",
      "         16443, 12026,  5035, 13587,  3778, 16244,  2571,  3209, 13600,  5029,\n",
      "          9085, 13322,  9173, 11344,  3382,  3064, 12776,  3682,  5051, 10253,\n",
      "          8926, 10983, 10534,  3807,  5037, 10862, 10877,  9472,  8574, 11088,\n",
      "         16565, 10695,  5019,  8523,  4189,  5097,  5485, 14596, 14621,  5074,\n",
      "          2099,  5147,  5052, 16959,  5817,  9779,  5040, 13430,  5149,  9305,\n",
      "          5035, 16171,  5051, 11246, 12738,  3742,  3659,  5345,  5019,  2361,\n",
      "         16443, 12026,  5105,  9833,  9246, 11597,  9223, 11329,  5816, 18642,\n",
      "         18602, 18551,  5062, 10695, 13658,  4112,  9790, 15748,  5072,  9594,\n",
      "         16030, 16565,  5040, 13587,  3209, 13600,  5029,  9085, 13322, 10534,\n",
      "          4189,  5097,  5485, 14596, 14621,  5074,  2099,  5147,  5052, 16959,\n",
      "          5817,  9779,  5040, 13430,  5149,  9305,  5035,  9371,  5016, 10999,\n",
      "         11578,  5166, 12738,  3659,  5345,  5019,  2361, 16443, 12026,  5105,\n",
      "          8988,  9833,  9472,  5008, 19185, 18629,  8455, 10983,  3742,  9711,\n",
      "          5040,  8936,  5331,  9046, 11922,  8494, 13909,  9957,  8736,  1923,\n",
      "          5278,  8455,  8745,  8591, 15420,  4565,  9439, 11825, 19184,  5412,\n",
      "          9020,     3],\n",
      "        [    2,  3659,  5345,  8799,  5093, 19509,  5040, 11799, 10695, 10026,\n",
      "          9012, 12877, 11330,  8961,  5244,  5096,  3245,  5008, 14585, 11081,\n",
      "          9046, 11329,  5816,  5019,  3778,  5213,  3245, 14635,  5029, 12115,\n",
      "          3898,  8574,  8523,  9711, 10984,  9931,  5138,  4090,  5377,  3576,\n",
      "         14544,  1932,  5189,  5284,  8888, 10322,  1923, 12831, 13906, 19017,\n",
      "         16171,  5051,  4647,  5441,  5043, 10322,  1998,  5389,  8936, 16565,\n",
      "          8456, 10695, 10026,  3030,  5466,  8538, 10464,  5105,  3322,  5550,\n",
      "          9568,  3940,  9449, 10555, 12183,  8529,  9594,     3,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    2, 11329,  5816,  5034,  9568,  9396,  5351,  3824,  5345,  5020,\n",
      "          8545,  4059,  5648, 15153,  2821, 15332, 10999, 11757, 10026,  3030,\n",
      "          5466,  8538, 10570,  5019, 14712,  5016,  3322,  5550,  5067, 13322,\n",
      "         10464,  9389,  9766,  3286,  5514,  5105,  3322,  5550,  9449,  9217,\n",
      "         12304, 11329,  5816,  9512,  8454,  3382,  5040,  5571,  5470, 13497,\n",
      "         19961,  8465,  9376,  8519, 18766,  5062, 10968,  3030,  5466,  8809,\n",
      "         17267,  5824,  5022,  8611,  2088,  5645,  9066,  8538, 11860, 17106,\n",
      "         19319,  4317,  5224, 14621, 12153, 10695, 10026,  3030,  5466,  8538,\n",
      "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert-SC\")\n",
    "\n",
    "# 토큰화 예제\n",
    "def tokenize_sentences(sentences):\n",
    "    # 문장들을 토큰화\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# 예시 문장 토큰화\n",
    "sample_sentences = df['summ_context'].tolist()[:5]  # 첫 5개 문장 예시\n",
    "tokenized_output = tokenize_sentences(sample_sentences)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"토큰화 결과:\")\n",
    "print(tokenized_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ko_bert 토크나이저를 쓰면 임베딩 잘 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핀 버트 토크나이저 + 로지스틱리그레션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['summ_context'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['summ_context'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")\n",
    "print(\"분류 리포트:\\n\", report)\n",
    "\n",
    "\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = BertTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert\", num_labels=2)\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_encodings = tokenizer(train_df['summ_context'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['summ_context'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# PyTorch Dataset 클래스 정의\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, y_train)\n",
    "test_dataset = NewsDataset(test_encodings, y_test)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,  # 로그를 더 자주 출력하도록 설정\n",
    "    evaluation_strategy=\"steps\",  # 평가 전략을 설정\n",
    "    eval_steps=100,  # 평가를 위한 스텝 수\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# 평가지표 함수 정의\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='binary')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# 조기 중단 콜백 추가\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Trainer를 통한 모델 훈련\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],  # 조기 중단 콜백 추가\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 예측 및 평가\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")\n",
    "print(\"분류 리포트:\\n\", report)\n",
    "\n",
    "\n",
    "# Step\tTraining Loss\tValidation Loss\tAccuracy\tF1\tPrecision\tRecall\n",
    "# 100\t0.673700\t0.682647\t0.580705\t0.058333\t0.466667\t0.031111\n",
    "# 200\t0.672600\t0.738892\t0.582560\t0.000000\t0.000000\t0.000000\n",
    "# 300\t0.655900\t0.697149\t0.573284\t0.361111\t0.481481\t0.288889\n",
    "# 400\t0.627100\t0.747127\t0.564007\t0.192440\t0.424242\t0.124444\n",
    "# 500\t0.434400\t0.982481\t0.554731\t0.329609\t0.443609\t0.262222\n",
    "# 600\t0.289800\t1.911037\t0.517625\t0.389671\t0.412935\t0.368889\n",
    "# 700\t0.190600\t2.163080\t0.556586\t0.366048\t0.453947\t0.306667\n",
    "# 800\t0.120500\t2.438479\t0.541744\t0.384040\t0.437500\t0.342222\n",
    "# 900\t0.075200\t2.769468\t0.551020\t0.434579\t0.458128\t0.413333\n",
    "# 1000\t0.040300\t3.182628\t0.560297\t0.328612\t0.453125\t0.257778\n",
    "# 1100\t0.053500\t3.084835\t0.530612\t0.423690\t0.434579\t0.413333\n",
    "# 1200\t0.019700\t3.810341\t0.552876\t0.244514\t0.414894\t0.173333\n",
    "# 1300\t0.017800\t3.791117\t0.545455\t0.350133\t0.434211\t0.293333\n",
    "# 1400\t0.030000\t3.805593\t0.551020\t0.366492\t0.445860\t0.311111\n",
    "# 1500\t0.048100\t3.526658\t0.551020\t0.431925\t0.457711\t0.408889\n",
    "# 1600\t0.003800\t3.807347\t0.536178\t0.407583\t0.436548\t0.382222\n",
    "# 1700\t0.002800\t3.892785\t0.551020\t0.473913\t0.463830\t0.484444\n",
    "# 1800\t0.014300\t3.804846\t0.558442\t0.466368\t0.470588\t0.462222\n",
    "# 1900\t0.016100\t3.954975\t0.538033\t0.430206\t0.443396\t0.417778\n",
    "# 2000\t0.006300\t4.082551\t0.504638\t0.483559\t0.428082\t0.555556\n",
    "# 2100\t0.001000\t4.174303\t0.536178\t0.338624\t0.418301\t0.284444\n",
    "# 2200\t0.000200\t4.217677\t0.530612\t0.412993\t0.432039\t0.395556\n",
    "# 2300\t0.011200\t4.336666\t0.517625\t0.462810\t0.432432\t0.497778\n",
    "# 2400\t0.001000\t4.378465\t0.532468\t0.444934\t0.441048\t0.448889\n",
    "# 2500\t0.001800\t4.347987\t0.541744\t0.434783\t0.448113\t0.422222\n",
    "# 2600\t0.000300\t4.394334\t0.541744\t0.399027\t0.440860\t0.364444\n",
    "# 2700\t0.009800\t4.398994\t0.549165\t0.362205\t0.442308\t0.306667\n",
    "# 2800\t0.000000\t4.432846\t0.556586\t0.359249\t0.452703\t0.297778\n",
    "# 2900\t0.000100\t4.375212\t0.536178\t0.436937\t0.442922\t0.431111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okt + 핀버트토크나이저 + 로지스틱리그레션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-4.csv'\n",
    "temp_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리한 파일이 'naver_news_origin_duplicates_summ3-5.csv'로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# okt 토큰화 된 리스트 -> 문자열로 전환\n",
    "temp_df['okt_tokenized_str'] = temp_df['okt_tokenized'].apply(lambda x: ''.join(x).strip('[]').replace(',', '').replace('\\'', ''))\n",
    "\n",
    "# 토큰화 데이터프레임 저장\n",
    "temp_df.to_csv('./datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv')\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"처리한 파일이 'naver_news_origin_duplicates_summ3-5.csv'로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.5529\n",
      "분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.86      0.69       314\n",
      "           1       0.39      0.12      0.19       225\n",
      "\n",
      "    accuracy                           0.55       539\n",
      "   macro avg       0.48      0.49      0.44       539\n",
      "weighted avg       0.50      0.55      0.48       539\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['okt_tokenized_str', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['okt_tokenized_str'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['okt_tokenized_str'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"정확도: {accuracy:.4f}\")\n",
    "print(\"분류 리포트:\\n\", report)\n",
    "\n",
    "# finbert 토크나이저만 적용한 점수\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n",
    "\n",
    "# Okt + finbert토크나이저 적용한 점수 (더 안좋아졌다.)\n",
    "# 정확도와 프리시전은 미미하게 낮아지고, 0에 대한 리콜과 f1은 오르고 1에대한 점수는 대폭 낮아졌다. \n",
    "# 정확도: 0.5529\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.86      0.69       314\n",
    "#            1       0.39      0.12      0.19       225\n",
    "\n",
    "#     accuracy                           0.55       539\n",
    "#    macro avg       0.48      0.49      0.44       539\n",
    "# weighted avg       0.50      0.55      0.48       539\n",
    "\n",
    "# 결론 : 0이라고 예측을 많이하고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 정확도: 0.5380\n",
      "XGBoost 분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67       314\n",
      "           1       0.39      0.18      0.25       225\n",
      "\n",
      "    accuracy                           0.54       539\n",
      "   macro avg       0.48      0.49      0.46       539\n",
      "weighted avg       0.50      0.54      0.49       539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['summ_context'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['summ_context'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "xgb_y_pred = xgb_model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost 정확도: {xgb_accuracy:.4f}\")\n",
    "print(\"XGBoost 분류 리포트:\\n\", xgb_report)\n",
    "\n",
    "\n",
    "# finbert토크나이저 + 로지스틱리그레션  적용한 점수\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n",
    "\n",
    "# Okt + finbert토크나이저 + 로지스틱리그레션 적용한 점수 (더 안좋아졌다.)\n",
    "# 정확도와 프리시전은 미미하게 낮아지고, 0에 대한 리콜과 f1은 오르고 1에대한 점수는 대폭 낮아졌다. \n",
    "# 정확도: 0.5529\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.86      0.69       314\n",
    "#            1       0.39      0.12      0.19       225\n",
    "\n",
    "#     accuracy                           0.55       539\n",
    "#    macro avg       0.48      0.49      0.44       539\n",
    "# weighted avg       0.50      0.55      0.48       539\n",
    "\n",
    "# finbert토크나이저 + XG부스팅\n",
    "# XGBoost 정확도: 0.5380\n",
    "# XGBoost 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.79      0.67       314\n",
    "#            1       0.39      0.18      0.25       225\n",
    "\n",
    "#     accuracy                           0.54       539\n",
    "#    macro avg       0.48      0.49      0.46       539\n",
    "# weighted avg       0.50      0.54      0.49       539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/moon_mys/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 정확도: 0.5603\n",
      "XGBoost 분류 리포트:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.84      0.69       314\n",
      "           1       0.43      0.16      0.24       225\n",
      "\n",
      "    accuracy                           0.56       539\n",
      "   macro avg       0.51      0.50      0.46       539\n",
      "weighted avg       0.52      0.56      0.50       539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-5.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['okt_tokenized_str', 'realUp']]\n",
    "\n",
    "# 데이터셋을 훈련셋과 테스트셋으로 분리\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "\n",
    "# 토큰화 및 패딩 함수 정의\n",
    "def tokenize_and_pad(sentences, max_length=512):\n",
    "    encodings = tokenizer(\n",
    "        sentences,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings['input_ids']\n",
    "\n",
    "# 훈련 및 테스트 데이터 토큰화\n",
    "X_train = tokenize_and_pad(train_df['okt_tokenized_str'].tolist())\n",
    "X_test = tokenize_and_pad(test_df['okt_tokenized_str'].tolist())\n",
    "y_train = train_df['realUp'].values\n",
    "y_test = test_df['realUp'].values\n",
    "\n",
    "# 텐서를 NumPy 배열로 변환\n",
    "X_train_np = X_train.detach().numpy()\n",
    "X_test_np = X_test.detach().numpy()\n",
    "\n",
    "# 로지스틱리그레션 모델은 피쳐를 418 받으므로 PCA를 사용하여 피처 수를 418개로 축소\n",
    "pca = PCA(n_components=418)\n",
    "X_train_reduced = pca.fit_transform(X_train_np)\n",
    "X_test_reduced = pca.transform(X_test_np)\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# 예측\n",
    "xgb_y_pred = xgb_model.predict(X_test_reduced)\n",
    "\n",
    "# 결과 평가\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_report = classification_report(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost 정확도: {xgb_accuracy:.4f}\")\n",
    "print(\"XGBoost 분류 리포트:\\n\", xgb_report)\n",
    "\n",
    "\n",
    "# finbert토크나이저 + 로지스틱리그레션  적용한 점수\n",
    "# 정확도: 0.5640\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.60      0.78      0.67       314\n",
    "#            1       0.46      0.27      0.34       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.53      0.52      0.51       539\n",
    "# weighted avg       0.54      0.56      0.53       539\n",
    "\n",
    "# Okt + finbert토크나이저 + 로지스틱리그레션\n",
    "# 정확도: 0.5529\n",
    "# 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.86      0.69       314\n",
    "#            1       0.39      0.12      0.19       225\n",
    "\n",
    "#     accuracy                           0.55       539\n",
    "#    macro avg       0.48      0.49      0.44       539\n",
    "# weighted avg       0.50      0.55      0.48       539\n",
    "\n",
    "# finbert토크나이저 + XG부스팅\n",
    "# XGBoost 정확도: 0.5380\n",
    "# XGBoost 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.79      0.67       314\n",
    "#            1       0.39      0.18      0.25       225\n",
    "\n",
    "#     accuracy                           0.54       539\n",
    "#    macro avg       0.48      0.49      0.46       539\n",
    "# weighted avg       0.50      0.54      0.49       539\n",
    "\n",
    "# Okt + finbert토크나이저 + XG부스팅\n",
    "# XGBoost 정확도: 0.5603\n",
    "# XGBoost 분류 리포트:\n",
    "#                precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.58      0.84      0.69       314\n",
    "#            1       0.43      0.16      0.24       225\n",
    "\n",
    "#     accuracy                           0.56       539\n",
    "#    macro avg       0.51      0.50      0.46       539\n",
    "# weighted avg       0.52      0.56      0.50       539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ollama RAG로 어찌 해볼순 없을까..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = '/home/alpaco/mys/projects/news/datas/네이버_뉴스기사/naver_news_origin_duplicates_summ3-3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "df = df[['summ_context', 'realUp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승세를 보이고 있습니다 코스피와 코스닥 모두 지난해 월 이후 최고치를 경신했으며 정부 정책 기대감과 월 효과 가 시장을 지지하고 있습니다 제약 바이오와 소프트웨어 부문에서 강세가 나타나고 있으며 외국인은 순매수로 전환되었습니다 전문가들은 원화의 강세에도 불구하고 수출 수요에 대한 긍정적인 전망을 유지하고 있습니다'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summ_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 임베딩 예시\n",
    "# import ollama\n",
    "# exmaple_embeddings = ollama.embeddings(\n",
    "#   prompt=df['summ_context'][0],\n",
    "#   model='mxbai-embed-large',\n",
    "# )\n",
    "# print(exmaple_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 함수화\n",
    "import ollama\n",
    "def get_embedding(prompt, model=\"mxbai-embed-large\"):\n",
    "    response = ollama.embeddings(prompt=prompt, model=model)\n",
    "    return response[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬랙션 생성\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collections = client.list_collections()\n",
    "wiki_exists = \"summ_context\" in [collection.name for collection in collections] # 컬렉션 존재 확인\n",
    "\n",
    "if wiki_exists:\n",
    "  client.delete_collection(\"summ_context\")\n",
    "  \n",
    "collection = client.create_collection(name=\"summ_context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 임베딩하고 컬렉션에 저장\n",
    "for i, d in enumerate(df['summ_context']):\n",
    "  embedding = get_embedding(d)\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=[embedding],\n",
    "    documents=[d]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승세를 보이고 있습니다 코스피와 코스닥 모두 지난해 월 이후 최고치를 경신했으며 정부 정책 기대감과 월 효과 가 시장을 지지하고 있습니다 제약 바이오와 소프트웨어 부문에서 강세가 나타나고 있으며 외국인은 순매수로 전환되었습니다 전문가들은 원화의 강세에도 불구하고 수출 수요에 대한 긍정적인 전망을 유지하고 있습니다'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summ_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'국내 증시는 긍정적인 시장 전망과 개인 투자자 및 외국인의 매수세로 인해 연초 상승세를 보이고 있습니다 코스피와 코스닥 모두 지난해 월 이후 최고치를 경신했으며 정부 정책 기대감과 월 효과 가 시장을 지지하고 있습니다 제약 바이오와 소프트웨어 부문에서 강세가 나타나고 있으며 외국인은 순매수로 전환되었습니다 전문가들은 원화의 강세에도 불구하고 수출 수요에 대한 긍정적인 전망을 유지하고 있습니다'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collection(\"summ_context\").get()['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬렉션 쿼리 함수화\n",
    "def query_collection(collection, embedding):\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedding],\n",
    "        n_results=1\n",
    "    )\n",
    "    return results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'박사방 유료 회원인 것으로 추정되는 대 남성이 한강에서 투신해 사망했습니다 그는 미성년자 여성들을 협박한 사건에 연루된 조주빈의 범죄로 인한 죄책감을 토로하는 유서를 남겼습니다 대구 제이미주병원에서 환자 및 직원 명이 코로나 양성 반응을 보였습니다 해당 병원은 집단감염이 발생한 대실요양병원과 같은 건물에 있었음에도 불구하고 환자에 대한 전수조사가 뒤늦게 이루어졌습니다 한진그룹 경영권 분쟁에서 조원태 회장이 최종 승리를 거두었으며 그의 사내이사 연임안이 찬성 로 가결되었습니다 한편 조현아 전 부사장 및 주도한 세력들의 반대안은 실패하였습니다 윤석열 검찰총장의 장모 최씨가 부동산 매입 과정에서 잔고증명서를 위조한 혐의로 불구속 기소되었습니다'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quetion = \"모두 주식이 오를 것으로 판단한 개인 투자자가 레버리지 효과를 얻기 위해 증권사로부터 빚을 내 투자하는 방식이다. 주가 상승기에는 많은 이익을 얻을 수 있지만, 반대로 하락할 때는 손실이 더 커진다. 주가 하락에 따른 손실은 물론 비싼 이자까지 내야 한다.\"\n",
    "embedding = get_embedding(quetion)\n",
    "data = query_collection(collection, embedding)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32074/3668068084.py:1: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  int(df.loc[df['summ_context'] == data,'realUp']) # 비슷한 뉴스를 찾아서 realUp을 가져온다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(df.loc[df['summ_context'] == data,'realUp']) # 비슷한 뉴스를 찾아서 realUp을 가져온다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon_mys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
