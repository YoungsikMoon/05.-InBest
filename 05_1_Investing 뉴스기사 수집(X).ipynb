{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 데이터 수집 (막힌 코드입니다. 접어주세요.. ㅠㅠ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30페이지 순회하며 뉴스 url이 있는 것에 대해서면 url과 타이틀을 가져온다.\n",
    "\n",
    "단 url에 접속하면 컨텐츠가 404인 경우가 있다. 이럴 경우 해당 raw는 삭제한다.\n",
    "\n",
    "404가 아닌경우엔 날짜 데이터까지 수집한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL, TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 초기 URL 설정\n",
    "base_url = \"https://kr.investing.com/equities/nvidia-corp-news\"\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# 30페이지까지 순회\n",
    "for page_num in range(1, 31):\n",
    "    # URL 생성\n",
    "    if page_num == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f\"{base_url}/{page_num}\"\n",
    "\n",
    "    # 웹페이지 가져오기\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup으로 HTML 파싱\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # 타겟 요소 선택\n",
    "    target_elements = soup.select(\"#__next > div.md\\\\:relative.md\\\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\\\[\\\\#232526\\\\].antialiased.xl\\\\:container.sm\\\\:px-6.md\\\\:grid-cols-\\\\[1fr_72px\\\\].md\\\\:gap-6.md\\\\:px-7.md\\\\:pt-10.md2\\\\:grid-cols-\\\\[1fr_420px\\\\].md2\\\\:gap-8.md2\\\\:px-8.xl\\\\:mx-auto.xl\\\\:gap-10.xl\\\\:px-10 > div.min-w-0 > div:nth-child(3) > ul > li > article > div > a\")\n",
    "\n",
    "    # 결과 저장\n",
    "    for element in target_elements:\n",
    "        href = element.get(\"href\")\n",
    "        if href:\n",
    "            href_value = \"https://kr.investing.com\" + href\n",
    "            title_value = element.text.strip()\n",
    "            results.append({\"url\": href_value, \"title\": title_value})\n",
    "            print(href_value, title_value)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(results, columns=[\"url\", \"title\"])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(\"nvidia_news.csv\", index_label=\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL, TITLE, CONTETNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 초기 URL 설정\n",
    "base_url = \"https://kr.investing.com/equities/nvidia-corp-news\"\n",
    "\n",
    "# 결과 저장 리스트\n",
    "results = []\n",
    "\n",
    "# 30페이지까지 순회\n",
    "for page_num in range(1, 31):\n",
    "    # URL 생성\n",
    "    if page_num == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f\"{base_url}/{page_num}\"\n",
    "\n",
    "    # 웹페이지 가져오기\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup으로 HTML 파싱\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # 타겟 요소 선택\n",
    "    target_elements = soup.select(\"#__next > div.md\\\\:relative.md\\\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\\\[\\\\#232526\\\\].antialiased.xl\\\\:container.sm\\\\:px-6.md\\\\:grid-cols-\\\\[1fr_72px\\\\].md\\\\:gap-6.md\\\\:px-7.md\\\\:pt-10.md2\\\\:grid-cols-\\\\[1fr_420px\\\\].md2\\\\:gap-8.md2\\\\:px-8.xl\\\\:mx-auto.xl\\\\:gap-10.xl\\\\:px-10 > div.min-w-0 > div:nth-child(3) > ul > li > article > div > a\")\n",
    "\n",
    "    # 결과 저장\n",
    "    for element in target_elements:\n",
    "        href = element.get(\"href\")\n",
    "        if href:\n",
    "            href_value = \"https://kr.investing.com\" + href\n",
    "            title_value = element.text.strip()\n",
    "\n",
    "            # 기사 내용 추출\n",
    "            article_response = requests.get(href_value)\n",
    "            article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "\n",
    "            # 기사 내용 처리\n",
    "            article_content = \"\"\n",
    "            article_elements = article_soup.select(\"#article > div > p, #article > div > span\")\n",
    "            if article_elements:\n",
    "                for article_element in article_elements:\n",
    "                    article_content += article_element.text.strip() + \" \"\n",
    "            else:\n",
    "                # 삭제된 기사 처리\n",
    "                error_element = article_soup.select(\"#leftColumn > div.error404\")\n",
    "                if error_element:\n",
    "                    print(f\"삭제된 기사: {href_value}\")\n",
    "                    continue\n",
    "\n",
    "            results.append({\"url\": href_value, \"title\": title_value, \"content\": article_content})\n",
    "            print(href_value, title_value, article_content)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(results, columns=[\"url\", \"title\", \"content\"])\n",
    "\n",
    "# 삭제된 기사 제외하고 CSV 파일로 저장\n",
    "df = df[df[\"content\"].notnull()]\n",
    "df.to_csv(\"nvidia_news.csv\", index_label=\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (최종 403 에러로 막힘..) 한국 뉴스 DATE TITLE CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 29.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 국가 설정 차라리 전체를 if문으로 하는게 나을지도...\n",
    "national_lnk = ['kr.',''] # 0: 한국 , 1: 미국\n",
    "filename = ['kr_','en_'] # 0: 한국 , 1: 미국\n",
    "\n",
    "# 오늘 날짜에 맞게 디렉터리 생성\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_path = f\"./datas/{today}/{filename[0]}nvidia_news.csv\"\n",
    "\n",
    "# 디렉터리 생성\n",
    "import os\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# 컬럼명을 지정하여 빈 데이터프레임 생성\n",
    "df = pd.DataFrame(columns=['url', \"date\", \"title\", \"content\"])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "# 초기 URL 설정\n",
    "base_url = \"https://{}investing.com/equities/nvidia-corp-news\".format(national_lnk[0])\n",
    "\n",
    "# 100페이지까지 순회\n",
    "for page_num in tqdm(range(1, 101)):\n",
    "\n",
    "    # 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # URL 생성\n",
    "    if page_num == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f\"{base_url}/{page_num}\"\n",
    "\n",
    "    # 웹페이지 가져오기\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup으로 HTML 파싱\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # 타겟 요소 선택\n",
    "    target_elements = soup.select(\"#__next > div.md\\\\:relative.md\\\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\\\[#232526\\\\].antialiased.xl\\\\:container.sm\\\\:px-6.md\\\\:grid-cols-\\\\[1fr_72px\\\\].md\\\\:gap-6.md\\\\:px-7.md\\\\:pt-10.md2\\\\:grid-cols-\\\\[1fr_420px\\\\].md2\\\\:gap-8.md2\\\\:px-8.xl\\\\:mx-auto.xl\\\\:gap-10.xl\\\\:px-10 > div.min-w-0 > div:nth-child(3) > ul > li > article > div > a\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    for element in target_elements:\n",
    "        try : \n",
    "            href = element.get(\"href\")\n",
    "            if href:\n",
    "                href_value = \"https://{}investing.com\".format(national_lnk[0]) + href\n",
    "                title_value = element.text.strip()\n",
    "\n",
    "                # 기사 내용 추출\n",
    "                article_response = requests.get(href_value)\n",
    "                article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "\n",
    "                # 기사 내용 처리\n",
    "                article_content = \"\"\n",
    "                article_elements = article_soup.select(\"#article > div > p, #article > div > span\")\n",
    "                if article_elements:\n",
    "                    for article_element in article_elements:\n",
    "                        article_content += article_element.text.strip() + \" \"\n",
    "                else:\n",
    "                    # 삭제된 기사 처리\n",
    "                    error_element = article_soup.select(\"#leftColumn > div.error404\")\n",
    "                    if error_element:\n",
    "                        print(f\"삭제된 기사: {href_value}\")\n",
    "                        continue\n",
    "                \n",
    "                # 줄바꿈 문자 제거\n",
    "                article_content = article_content.replace(\"\\n\", \" \")\n",
    "\n",
    "                # 날짜 정보 추출\n",
    "                date_element = article_soup.select_one(\"#__next > div.md\\\\:relative.md\\\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\\\[\\\\#232526\\\\].antialiased.xl\\\\:container.sm\\\\:px-6.md\\\\:grid-cols-\\\\[1fr_72px\\\\].md\\\\:gap-6.md\\\\:px-7.md\\\\:pt-10.md2\\\\:grid-cols-\\\\[1fr_420px\\\\].md2\\\\:gap-8.md2\\\\:px-8.xl\\\\:mx-auto.xl\\\\:gap-10.xl\\\\:px-10 > div.min-w-0 > div > div:nth-child(1) > div.relative.flex.flex-col > div.mx-0.mt-1 > div.mt-2.flex.flex-col.gap-2.text-xs.md\\\\:mt-2\\\\.5.md\\\\:gap-2\\\\.5 > div > div > span\")\n",
    "                if date_element:\n",
    "                    date_value = date_element.text[3:17].strip().replace(\" \", \"\")\n",
    "                else:\n",
    "                    date_value = \"\"\n",
    "\n",
    "                if article_content:  # 기사 내용이 있는 경우에만 결과에 추가 *뉴스 사이트 프로구독권 결제 한 계정만 보이는 뉴스가 있기 때문.\n",
    "                    results.append({\"url\": href_value, \"date\": date_value, \"title\": title_value, \"content\": article_content})\n",
    "                    print(page_num, date_value, title_value, article_content)\n",
    "        except : pass\n",
    "        \n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV 파일에 데이터 저장\n",
    "    df.to_csv(save_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (최종 403 에러로 막힘..) 해외 뉴스 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "페이지마다 csv 업데이트 되도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:00<00:01, 58.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:00<00:01, 58.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:00<00:01, 59.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:00<00:00, 60.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:00<00:00, 57.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:01<00:00, 56.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:01<00:00, 58.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [00:01<00:00, 59.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 58.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n",
      "<Response [403]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 국가 설정 차라리 전체를 if문으로 하는게 나을지도...\n",
    "national_lnk = ['kr.',''] # 0: 한국 , 1: 미국\n",
    "filename = ['kr_','en_'] # 0: 한국 , 1: 미국\n",
    "\n",
    "# 오늘 날짜에 맞게 디렉터리 생성\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_path = f\"./datas/{today}/{filename[1]}nvidia_news.csv\"\n",
    "\n",
    "# 디렉터리 생성\n",
    "import os\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# 컬럼명을 지정하여 빈 데이터프레임 생성\n",
    "df = pd.DataFrame(columns=['url', \"date\", \"title\", \"content\"])\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "# 초기 URL 설정\n",
    "base_url = \"https://{}investing.com/equities/nvidia-corp-news\".format(national_lnk[1])\n",
    "\n",
    "# 페이지까지 순회\n",
    "for page_num in tqdm(range(1, 2)):\n",
    "    # 결과 저장 리스트\n",
    "    results = []\n",
    "\n",
    "    # URL 생성\n",
    "    if page_num == 1:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = f\"{base_url}/{page_num}\"\n",
    "\n",
    "    headers = {\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n",
    "        \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Connection\": \"Upgrade\",\n",
    "        \"Host\": \"streaming.forexpros.com\",\n",
    "        \"Origin\": \"https://www.investing.com\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Sec-Websocket-Extensions\": \"permessage-deflate; client_max_window_bits\",\n",
    "        \"Sec-Websocket-Key\": \"yPUXVYuGSS8jh2A35n/E4w==\",\n",
    "        \"Sec-Websocket-Version\": \"13\",\n",
    "        \"Upgrade\": \"websocket\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # 웹페이지 가져오기\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response)\n",
    "    \"\"\"\n",
    "    # BeautifulSoup으로 HTML 파싱\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # 타겟 요소 선택\n",
    "    target_elements = soup.select(\"#__next > div.md\\\\:relative.md\\\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\\\[\\\\#232526\\\\].antialiased.xl\\\\:container.sm\\\\:px-6.md\\\\:grid-cols-\\\\[1fr_72px\\\\].md\\\\:gap-6.md\\\\:px-7.md\\\\:pt-10.md2\\\\:grid-cols-\\\\[1fr_420px\\\\].md2\\\\:gap-8.md2\\\\:px-8.xl\\\\:mx-auto.xl\\\\:gap-10.xl\\\\:px-10 > div.min-w-0 > div:nth-child(3) > ul > li > article > div > a\")\n",
    "\n",
    "    # 결과 저장\n",
    "    for element in target_elements:\n",
    "        try:\n",
    "            href = element.get(\"href\")\n",
    "            if href:\n",
    "                href_value = \"https://{}investing.com\".format(national_lnk[1]) + href\n",
    "                title_value = element.text.strip()\n",
    "\n",
    "                # 기사 내용 추출\n",
    "                article_response = requests.get(href_value)\n",
    "                article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "\n",
    "                # 기사 내용 처리\n",
    "                article_content = \"\"\n",
    "                article_elements = article_soup.select(\"#article > div > p, #article > div > span\")\n",
    "                if article_elements:\n",
    "                    for article_element in article_elements:\n",
    "                        article_content += article_element.text.strip() + \" \"\n",
    "                else:\n",
    "                    # 삭제된 기사 처리\n",
    "                    error_element = article_soup.select(\"#leftColumn > div.error404\")\n",
    "                    if error_element:\n",
    "                        print(f\"삭제된 기사: {href_value}\")\n",
    "                        continue\n",
    "\n",
    "                # 줄바꿈 문자 제거\n",
    "                article_content = article_content.replace(\"\\n\", \" \")\n",
    "\n",
    "                # 날짜 정보 추출\n",
    "                date_element = article_soup.select_one(\"#__next > div.md\\:relative.md\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\[\\#232526\\].antialiased.xl\\:container.sm\\:px-6.md\\:grid-cols-\\[1fr_72px\\].md\\:gap-6.md\\:px-7.md\\:pt-10.md2\\:grid-cols-\\[1fr_420px\\].md2\\:gap-8.md2\\:px-8.xl\\:mx-auto.xl\\:gap-10.xl\\:px-10 > div.min-w-0 > div > div > div.relative.flex.flex-col > div.mx-0.mt-1 > div.mt-2.flex.flex-col.gap-2.text-xs.md\\:mt-2\\.5.md\\:gap-2\\.5 > div > div:nth-child(2) > span:nth-child(2)\")\n",
    "                if date_element:\n",
    "                    date_str = date_element.text[8:18].strip().replace(\" \", \"\")\n",
    "                    date_value = datetime.datetime.strptime(date_str, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "                else:\n",
    "                    date_element = article_soup.select_one(\"#__next > div.md\\:relative.md\\:bg-white > div.relative.flex > div.grid.flex-1.grid-cols-1.px-4.pt-5.font-sans-v2.text-\\[\\#232526\\].antialiased.xl\\:container.sm\\:px-6.md\\:grid-cols-\\[1fr_72px\\].md\\:gap-6.md\\:px-7.md\\:pt-10.md2\\:grid-cols-\\[1fr_420px\\].md2\\:gap-8.md2\\:px-8.xl\\:mx-auto.xl\\:gap-10.xl\\:px-10 > div.min-w-0 > div > div > div.relative.flex.flex-col > div.mx-0.mt-1 > div.mt-2.flex.flex-col.gap-2.text-xs.md\\:mt-2\\.5.md\\:gap-2\\.5 > div > div > span\")\n",
    "                    date_str = date_element.text[9:20].strip().replace(\" \", \"\")\n",
    "                    date_value = datetime.datetime.strptime(date_str, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                if article_content:  # 기사 내용이 있는 경우에만 결과에 추가 *뉴스 사이트 프로구독권 결제 한 계정만 보이는 뉴스가 있기 때문.\n",
    "                    results.append({\"url\": href_value, \"date\": date_value, \"title\": title_value, \"content\": article_content})\n",
    "                    print(page_num, date_value, title_value, article_content)\n",
    "        except : pass\n",
    "\n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # CSV 파일에 데이터 저장\n",
    "    df.to_csv(save_path, mode='a', header=False, index=False) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moon_mys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
